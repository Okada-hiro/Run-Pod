# /workspace/new_new_main.py
# Final Speed Tuning: Aggressive VAD + In-Memory TTS

import uvicorn
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
import torch
import numpy as np
import asyncio
import logging
import sys
import os
import io
import re
import scipy.io.wavfile as wavfile # ãƒ¡ãƒ¢ãƒªæ›¸ãå‡ºã—ç”¨

# --- ãƒ­ã‚®ãƒ³ã‚°è¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# --- å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
try:
    from transcribe_func import GLOBAL_ASR_MODEL_INSTANCE
    from supporter_generator import generate_answer_stream
    # TTSã®synthesize_speechã¯ä½¿ã‚ãšã€ãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥å©ããŸã‚ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸è¦ã ãŒ
    # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã®ãŸã‚ã« new_text_to_speech ã‚’å‚ç…§
    import new_text_to_speech as tts_module
    from new_speaker_filter import SpeakerGuard
except ImportError as e:
    logger.error(f"[ERROR] å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    sys.exit(1)

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š ---
PROCESSING_DIR = "incoming_audio"
os.makedirs(PROCESSING_DIR, exist_ok=True)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"Using Device: {DEVICE}")

app = FastAPI()
app.mount(f"/download", StaticFiles(directory=PROCESSING_DIR), name="download")

# SpeakerGuardåˆæœŸåŒ–
speaker_guard = SpeakerGuard()
NEXT_AUDIO_IS_REGISTRATION = False

# --- Silero VAD ã®ãƒ­ãƒ¼ãƒ‰ ---
logger.info("â³ Loading Silero VAD model...")
try:
    vad_model, utils = torch.hub.load(
        repo_or_dir='snakers4/silero-vad',
        model='silero_vad',
        force_reload=False,
        onnx=False
    )
    (get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils
    vad_model.to(DEVICE)
    logger.info("âœ… Silero VAD model loaded.")
except Exception as e:
    logger.critical(f"Silero VAD Load Failed: {e}")
    sys.exit(1)


# --- API: ç™»éŒ²ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ ---
@app.post("/enable-registration")
async def enable_registration():
    global NEXT_AUDIO_IS_REGISTRATION
    NEXT_AUDIO_IS_REGISTRATION = True
    logger.info("ã€ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ã€‘æ¬¡ã®ç™ºè©±ã‚’æ–°è¦è©±è€…ã¨ã—ã¦ç™»éŒ²ã—ã¾ã™")
    return {"message": "ç™»éŒ²ãƒ¢ãƒ¼ãƒ‰å¾…æ©Ÿä¸­"}


# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼: éŸ³å£°å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ---
async def process_voice_pipeline(audio_float32_np, websocket: WebSocket, chat_history: list):
    global NEXT_AUDIO_IS_REGISTRATION
    
    # SpeakerGuardç”¨ã« TensoråŒ– (1, samples)
    voice_tensor = torch.from_numpy(audio_float32_np).float().unsqueeze(0)

    # ---------------------------
    # 0. è©±è€…ç™»éŒ²ãƒ¢ãƒ¼ãƒ‰
    # ---------------------------
    if NEXT_AUDIO_IS_REGISTRATION:
        temp_reg_path = f"{PROCESSING_DIR}/reg_{id(audio_float32_np)}.wav"
        import soundfile as sf
        sf.write(temp_reg_path, audio_float32_np, 16000)
        
        success = await asyncio.to_thread(speaker_guard.register_new_speaker, temp_reg_path)
        NEXT_AUDIO_IS_REGISTRATION = False
        
        if success:
            await websocket.send_json({"status": "ignored", "message": "âœ… æ–°ã—ã„ãƒ¡ãƒ³ãƒãƒ¼ã‚’ç™»éŒ²ã—ã¾ã—ãŸï¼"})
        else:
            await websocket.send_json({"status": "error", "message": "ç™»éŒ²ã«å¤±æ•—ã—ã¾ã—ãŸ"})
        return

    # ---------------------------
    # 1. è©±è€…èªè­˜ (SpeakerGuard)
    # ---------------------------
    is_allowed = await asyncio.to_thread(speaker_guard.verify_tensor, voice_tensor)

    if not is_allowed:
        logger.info("[Access Denied] ç™»éŒ²ã•ã‚Œã¦ã„ãªã„è©±è€…ã§ã™ã€‚")
        await websocket.send_json({"status": "ignored", "message": "ğŸš« æœªç™»éŒ²ã®å£°ã§ã™ (ãƒ–ãƒ­ãƒƒã‚¯)"})
        return

    # ---------------------------
    # 2. Whisper æ–‡å­—èµ·ã“ã—
    # ---------------------------
    try:
        if GLOBAL_ASR_MODEL_INSTANCE is None:
            raise ValueError("Whisper Model not loaded")

        # faster-whisper ã¯ numpy array ã‚’ç›´æ¥å—ã‘å–ã‚Œã‚‹
        segments = await asyncio.to_thread(
            GLOBAL_ASR_MODEL_INSTANCE.transcribe, 
            audio_float32_np
        )
        
        text = "".join([s[2] for s in GLOBAL_ASR_MODEL_INSTANCE.ts_words(segments)])
        
        if not text.strip():
            # logger.info("[TASK] ç©ºã®èªè­˜çµæœ")
            return

        logger.info(f"ğŸ“ èªè­˜: {text}")
        await websocket.send_json({
            "status": "transcribed",
            "question_text": text
        })

        # ---------------------------
        # 3. LLM & TTS ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
        # ---------------------------
        await handle_llm_tts(text, websocket, chat_history)

    except Exception as e:
        logger.error(f"Pipeline Error: {e}", exc_info=True)
        await websocket.send_json({"status": "error", "message": "å‡¦ç†ã‚¨ãƒ©ãƒ¼"})


# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼: å›ç­”ç”Ÿæˆã¨éŸ³å£°åˆæˆ (In-Memoryé«˜é€ŸåŒ–ç‰ˆ) ---
async def handle_llm_tts(text: str, websocket: WebSocket, chat_history: list):
    text_buffer = ""
    sentence_count = 0
    full_answer = ""
    # å¥èª­ç‚¹ã§ç´°ã‹ãåŒºåˆ‡ã‚‹
    split_pattern = r'(?<=[ã€‚ï¼ï¼Ÿ\nã€])'

    iterator = generate_answer_stream(text, history=chat_history)

    # â˜…é«˜é€ŸåŒ–: ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ãƒ¡ãƒ¢ãƒªä¸Šã§WAVç”Ÿæˆ
    async def send_audio_chunk_memory(phrase):
        if not phrase: return
        try:
            # TTSãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥å‘¼ã³å‡ºã—
            # new_text_to_speech.py ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            model = tts_module.GLOBAL_TTS_MODEL
            spk_id = tts_module.GLOBAL_SPEAKER_ID
            
            if model is None: return

            # æ¨è«– (GPU)
            sr, audio_data = await asyncio.to_thread(
                model.infer,
                text=phrase,
                speaker_id=spk_id,
                style="Neutral",
                style_weight=0.5, # å°‘ã—å¼±ã‚ã¦é€Ÿåº¦å„ªå…ˆ
                sdp_ratio=0.2,
                noise=0.6,
                noise_w=0.8,
                length=1.0
            )
            
            # Int16å¤‰æ›
            if audio_data.dtype != np.int16:
                audio_norm = audio_data / np.abs(audio_data).max()
                audio_int16 = (audio_norm * 32767).astype(np.int16)
            else:
                audio_int16 = audio_data

            # ãƒ¡ãƒ¢ãƒªä¸Šã§WAVåŒ– (BytesIO)
            mem_file = io.BytesIO()
            wavfile.write(mem_file, sr, audio_int16)
            wav_bytes = mem_file.getvalue()
            
            # é€ä¿¡
            await websocket.send_bytes(wav_bytes)

        except Exception as e:
            logger.error(f"TTS Gen Error: {e}")

    try:
        for chunk in iterator:
            text_buffer += chunk
            full_answer += chunk
            
            if full_answer.strip() == "[SILENCE]":
                await websocket.send_json({"status": "ignored", "message": "ï¼ˆå¿œç­”ãªã—ï¼‰"})
                return

            sentences = re.split(split_pattern, text_buffer)
            if len(sentences) > 1:
                for sent in sentences[:-1]:
                    if sent.strip():
                        sentence_count += 1
                        await websocket.send_json({"status": "reply_chunk", "text_chunk": sent})
                        # ãƒ¡ãƒ¢ãƒªçµŒç”±ã§é€ä¿¡
                        await send_audio_chunk_memory(sent)
                text_buffer = sentences[-1]
        
        if text_buffer.strip():
            sentence_count += 1
            await websocket.send_json({"status": "reply_chunk", "text_chunk": text_buffer})
            await send_audio_chunk_memory(text_buffer)

        chat_history.append({"role": "user", "parts": [text]})
        chat_history.append({"role": "model", "parts": [full_answer]})
        
        await websocket.send_json({"status": "complete", "answer_text": full_answer})

    except Exception as e:
        logger.error(f"LLM/TTS Error: {e}")


# ---------------------------
# WebSocket ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ (é¬¼VADè¨­å®š)
# ---------------------------
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    logger.info("[WS] Client Connected.")
    
    # â˜…â˜…â˜… æœ€é‡è¦ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç®‡æ‰€ â˜…â˜…â˜…
    # threshold: 0.8 (è‡ªä¿¡åº¦80%æœªæº€ã¯ãƒã‚¤ã‚ºã¨ã¿ãªã™ã€‚ã“ã‚Œã§æ¯é£ã„ã‚’é™¤å»)
    # min_silence_duration_ms: 200 (200msç„¡éŸ³ãªã‚‰å³çµ‚äº†ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ100ã ã¨æ—©ã™ãã‚‹å ´åˆãŒã‚ã‚‹ã®ã§å¾®èª¿æ•´)
    # speech_pad_ms: 10 (ä½™è¨ˆãªä½™éŸ»ã‚’ã¤ã‘ãªã„)
    vad_iterator = VADIterator(
        vad_model, 
        threshold=0.8, 
        min_silence_duration_ms=200, 
        speech_pad_ms=10
    )
    
    audio_buffer = [] 
    is_speaking = False
    
    WINDOW_SIZE_SAMPLES = 512 
    SAMPLE_RATE = 16000
    chat_history = []

    try:
        while True:
            # 1. å—ä¿¡
            data_bytes = await websocket.receive_bytes()
            audio_chunk_np = np.frombuffer(data_bytes, dtype=np.float32).copy()
            
            # 2. 512ã‚µãƒ³ãƒ—ãƒ«åˆ†å‰²ãƒ«ãƒ¼ãƒ—
            offset = 0
            while offset + WINDOW_SIZE_SAMPLES <= len(audio_chunk_np):
                window_np = audio_chunk_np[offset : offset + WINDOW_SIZE_SAMPLES]
                offset += WINDOW_SIZE_SAMPLES
                
                # TensoråŒ–
                window_tensor = torch.from_numpy(window_np).unsqueeze(0).to(DEVICE)

                # VADåˆ¤å®š
                speech_dict = await asyncio.to_thread(vad_iterator, window_tensor, return_seconds=True)
                
                if speech_dict:
                    if "start" in speech_dict:
                        logger.info("ğŸ—£ï¸ Start")
                        is_speaking = True
                        await websocket.send_json({"status": "processing", "message": "ğŸ‘‚ èã„ã¦ã„ã¾ã™..."})
                        audio_buffer = [window_np] 
                    
                    elif "end" in speech_dict:
                        logger.info("ğŸ¤« End (Cut!)") # å³ã‚«ãƒƒãƒˆãƒ­ã‚°
                        if is_speaking:
                            is_speaking = False
                            audio_buffer.append(window_np)
                            
                            full_audio = np.concatenate(audio_buffer)
                            
                            # ãƒã‚¤ã‚ºåˆ¤å®š (0.2ç§’ä»¥ä¸‹ã¯ç„¡è¦–)
                            if len(full_audio) / SAMPLE_RATE < 0.2:
                                await websocket.send_json({"status": "ignored", "message": "..."})
                            else:
                                await websocket.send_json({"status": "processing", "message": "ğŸ§  AIæ€è€ƒä¸­..."})
                                await process_voice_pipeline(full_audio, websocket, chat_history)
                            
                            audio_buffer = [] 
                
                else:
                    if is_speaking:
                        audio_buffer.append(window_np)

    except WebSocketDisconnect:
        logger.info("[WS] Disconnected")
    except Exception as e:
        logger.error(f"[WS ERROR] {e}", exc_info=True)
    finally:
        vad_iterator.reset_states()


# ---------------------------
# ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰
# ---------------------------
@app.get("/", response_class=HTMLResponse)
async def get_root():
    return """
    <!DOCTYPE html>
    <html lang="ja">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device.width, initial-scale=1.0">
        <title>Ultra Fast AI Talk</title>
        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; display: grid; place-items: center; min-height: 90vh; background: #202c33; color: #e9edef; margin: 0; }
            #container { background: #111b21; padding: 0; border-radius: 0; text-align: center; width: 100%; max-width: 600px; height: 100vh; display: flex; flex-direction: column; }
            @media (min-width: 600px) { #container { height: 90vh; border-radius: 12px; } }
            
            header { background: #202c33; padding: 15px; border-bottom: 1px solid #374045; font-weight: bold; font-size: 1.1rem; display: flex; justify-content: space-between; align-items: center; }
            #chat-box { flex: 1; overflow-y: auto; padding: 20px; background-color: #0b141a; }
            .row { display: flex; width: 100%; margin-bottom: 8px; }
            .row.ai { justify-content: flex-start; }
            .row.user { justify-content: flex-end; }
            .bubble { padding: 8px 12px; border-radius: 8px; max-width: 75%; font-size: 0.95rem; line-height: 1.4; word-wrap: break-word; }
            .ai .bubble { background: #202c33; color: #e9edef; border-top-left-radius: 0; }
            .user .bubble { background: #005c4b; color: #e9edef; border-top-right-radius: 0; }
            #controls { background: #202c33; padding: 15px; border-top: 1px solid #374045; }
            button { padding: 10px 20px; border-radius: 24px; border: none; font-size: 1rem; cursor: pointer; margin: 0 5px; font-weight: bold; }
            #btn-start { background: #00a884; color: #fff; }
            #btn-stop { background: #ef5350; color: #fff; display: none; }
            #btn-register { background: #3b4a54; color: #fff; font-size: 0.8rem; padding: 8px 15px; }
            #status { margin-bottom: 10px; font-size: 0.9rem; color: #8696a0; min-height: 1.2em; }
        </style>
    </head>
    <body>
        <div id="container">
            <header>
                <span>AI Agent âš¡</span>
                <button id="btn-register">ï¼‹ ãƒ¡ãƒ³ãƒãƒ¼è¿½åŠ </button>
            </header>
            <div id="chat-box"></div>
            <div id="controls">
                <div id="status">æ¥ç¶šå¾…æ©Ÿä¸­...</div>
                <button id="btn-start">ä¼šè©±ã‚’å§‹ã‚ã‚‹</button>
                <button id="btn-stop">çµ‚äº†ã™ã‚‹</button>
            </div>
        </div>
        <script>
            let socket;
            let audioContext;
            let processor;
            let source;
            let isRecording = false;
            const btnStart = document.getElementById('btn-start');
            const btnStop = document.getElementById('btn-stop');
            const btnRegister = document.getElementById('btn-register');
            const statusDiv = document.getElementById('status');
            const chatBox = document.getElementById('chat-box');
            let audioQueue = [];
            let isPlaying = false;
            let currentAiBubble = null;

            function logChat(role, text) {
                const row = document.createElement('div');
                row.className = `row ${role}`;
                const bubble = document.createElement('div');
                bubble.className = 'bubble';
                bubble.textContent = text;
                row.appendChild(bubble);
                chatBox.appendChild(row);
                chatBox.scrollTop = chatBox.scrollHeight;
                return bubble;
            }

            btnRegister.onclick = async () => {
                try {
                    await fetch('/enable-registration', { method: 'POST' });
                    statusDiv.textContent = "ğŸ†• æ–°è¦ãƒ¡ãƒ³ãƒãƒ¼ç™»éŒ²ãƒ¢ãƒ¼ãƒ‰";
                    statusDiv.style.color = "#00a884";
                } catch(e) {}
            };

            async function startRecording() {
                try {
                    statusDiv.textContent = "æ¥ç¶šä¸­...";
                    const wsProtocol = window.location.protocol === 'https:' ? 'wss://' : 'ws://';
                    socket = new WebSocket(wsProtocol + window.location.host + '/ws');
                    socket.binaryType = 'arraybuffer';
                    socket.onopen = async () => {
                        console.log("WS Connected");
                        statusDiv.textContent = "ğŸ™ï¸ æº–å‚™OK";
                        statusDiv.style.color = "#e9edef";
                        btnStart.style.display = 'none';
                        btnStop.style.display = 'inline-block';
                        await initAudioStream();
                    };
                    socket.onmessage = async (event) => {
                        if (event.data instanceof ArrayBuffer) {
                            audioQueue.push(event.data);
                            processAudioQueue();
                        } else {
                            const data = JSON.parse(event.data);
                            if (data.status === 'processing') {
                                statusDiv.textContent = data.message;
                                if (data.message.includes("èã„ã¦")) statusDiv.style.color = "#ef5350"; 
                                else if (data.message.includes("æ€è€ƒä¸­")) statusDiv.style.color = "#00a884";
                            }
                            if (data.status === 'transcribed') logChat('user', data.question_text);
                            if (data.status === 'reply_chunk') {
                                if (!currentAiBubble) currentAiBubble = logChat('ai', '');
                                currentAiBubble.textContent += data.text_chunk;
                                chatBox.scrollTop = chatBox.scrollHeight;
                            }
                            if (data.status === 'complete') {
                                if (!currentAiBubble && data.answer_text) logChat('ai', data.answer_text);
                                currentAiBubble = null;
                                statusDiv.textContent = "ğŸ™ï¸ æº–å‚™OK";
                                statusDiv.style.color = "#e9edef";
                            }
                            if (data.status === 'ignored') statusDiv.textContent = data.message;
                        }
                    };
                    socket.onclose = () => stopRecording();
                } catch (e) { statusDiv.textContent = "æ¥ç¶šã‚¨ãƒ©ãƒ¼"; }
            }

            async function initAudioStream() {
                audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
                const stream = await navigator.mediaDevices.getUserMedia({ audio: { channelCount: 1, echoCancellation: true, noiseSuppression: true, autoGainControl: true } });
                source = audioContext.createMediaStreamSource(stream);
                processor = audioContext.createScriptProcessor(4096, 1, 1);
                processor.onaudioprocess = (e) => {
                    if (!socket || socket.readyState !== WebSocket.OPEN) return;
                    socket.send(e.inputBuffer.getChannelData(0).buffer);
                };
                source.connect(processor);
                processor.connect(audioContext.destination);
                isRecording = true;
            }

            function stopRecording() {
                isRecording = false;
                if (source) source.disconnect();
                if (processor) processor.disconnect();
                if (audioContext) audioContext.close();
                if (socket) socket.close();
                btnStart.style.display = 'inline-block';
                btnStop.style.display = 'none';
                statusDiv.textContent = "åœæ­¢ä¸­";
            }

            async function processAudioQueue() {
                if (isPlaying || audioQueue.length === 0) return;
                isPlaying = true;
                const wavData = audioQueue.shift();
                try {
                    if (!audioContext || audioContext.state === 'closed') audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    const audioBuffer = await audioContext.decodeAudioData(wavData);
                    const source = audioContext.createBufferSource();
                    source.buffer = audioBuffer;
                    source.connect(audioContext.destination);
                    source.onended = () => { isPlaying = false; processAudioQueue(); };
                    source.start(0);
                } catch(e) { isPlaying = false; }
            }

            btnStart.onclick = startRecording;
            btnStop.onclick = stopRecording;
        </script>
    </body>
    </html>
    """

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)