# /workspace/new_new_main.py (å®Œå…¨ä¿®æ­£ç‰ˆ: Web Audio APIå¯¾å¿œ)
import uvicorn
from fastapi import FastAPI, WebSocket, Request
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.websockets import WebSocketDisconnect
import os
import asyncio
import time
import logging 
import sys 
from pydub import AudioSegment
import io
import re
from speaker_filter import SpeakerGuard



# --- ãƒ­ã‚®ãƒ³ã‚°è¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)] 
)
logger = logging.getLogger(__name__)

# --- å‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
try:
    from transcribe_func import whisper_text_only
    # supporter_generator ã‚’å„ªå…ˆã—ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    try:
        from supporter_generator import generate_answer_stream
    except ImportError:
        from new_answer_generator import generate_answer_stream

    from new_text_to_speech import synthesize_speech
except ImportError as e:
    print(f"[ERROR] å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")

# --- è¨­å®š ---
PROCESSING_DIR = "incoming_audio" 
LANGUAGE = "ja"

# --- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ– ---
app = FastAPI()
os.makedirs(PROCESSING_DIR, exist_ok=True)
app.mount(f"/download", StaticFiles(directory=PROCESSING_DIR), name="download")


# ---------------------------
# 1. æ–‡ã”ã¨ã®å‡¦ç†é–¢æ•°
# ---------------------------
async def process_sentence(text: str, base_filename: str, index: int, websocket: WebSocket):
    logger.info(f"[STREAM] æ–‡{index}: {text[:20]}...")
    
    # (A) å­—å¹•é€ä¿¡
    try:
        await websocket.send_json({
            "status": "reply_chunk",
            "text_chunk": text
        })
    except Exception as e:
        logger.error(f"[STREAM ERROR] ãƒ†ã‚­ã‚¹ãƒˆé€ä¿¡å¤±æ•—: {e}")

    # (B) éŸ³å£°åˆæˆ
    part_filename = f"{base_filename}.part{index}.wav"
    part_path_abs = os.path.abspath(os.path.join(PROCESSING_DIR, part_filename))

    success = await asyncio.to_thread(
        synthesize_speech,
        text_to_speak=text,
        output_wav_path=part_path_abs
    )
    
    if success:
        try:
            # (C) éŸ³å£°ãƒã‚§ãƒƒã‚¯ã¨å¤‰æ›
            audio_segment = AudioSegment.from_wav(part_path_abs)
            duration_sec = len(audio_segment) / 1000.0
            
            if duration_sec < 0.1:
                logger.warning(f"âš ï¸ [AUDIO WARNING] ç”ŸæˆéŸ³å£°ãŒçŸ­ã™ãã¾ã™: {duration_sec}ç§’")
            else:
                logger.info(f"ğŸ”Š [AUDIO OK] æ–‡{index} é•·ã•: {duration_sec}ç§’")

            mp3_buffer = io.BytesIO()
            audio_segment.export(mp3_buffer, format="mp3", bitrate="128k")
            audio_data = mp3_buffer.getvalue()

            # (D) é€ä¿¡
            await websocket.send_bytes(audio_data)
        except Exception as e:
            logger.error(f"[STREAM ERROR] éŸ³å£°å¤‰æ›ãƒ»é€ä¿¡ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)


# ---------------------------
# 2. ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç† (ãƒ¡ã‚¤ãƒ³ãƒ•ãƒ­ãƒ¼)
# ---------------------------
async def process_audio_file(audio_path: str, original_filename: str, websocket: WebSocket, chat_history: list):
    logger.info(f"[TASK START] ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {original_filename}")
    # â˜…â˜…â˜… ã“ã“ã«è¿½åŠ : è©±è€…åˆ¤å®š â˜…â˜…â˜…
    # æœ¬äººã˜ã‚ƒãªã‘ã‚Œã°å³çµ‚äº† (Whisperã‚‚LLMã‚‚å›ã•ãªã„)
    is_owner = await asyncio.to_thread(speaker_guard.is_owner, audio_path)
    
    if not is_owner:
        logger.info("[TASK] ä»–äººã®å£°ã®ãŸã‚ç„¡è¦–ã—ã¾ã—ãŸ")
        await websocket.send_json({"status": "ignored", "message": "ï¼ˆä»–äººã®å£°ã‚’ç„¡è¦–ï¼‰"})
        return 
    # â˜…â˜…â˜… ã“ã“ã¾ã§ â˜…â˜…â˜…
    try:
        # --- æ–‡å­—èµ·ã“ã— ---
        output_txt_path = os.path.join(PROCESSING_DIR, original_filename + ".txt")
        
        question_text = await asyncio.to_thread(
            whisper_text_only,
            audio_path, language=LANGUAGE, output_txt=output_txt_path
        )
        logger.info(f"[TASK] æ–‡å­—èµ·ã“ã—å®Œäº†: {question_text}")

        await websocket.send_json({
            "status": "transcribed",
            "message": "...",
            "question_text": question_text
        })

        # --- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å›ç­” & ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç† ---
        text_buffer = ""
        sentence_count = 0
        full_answer_log = ""
        split_pattern = r'(?<=[ã€‚ï¼ï¼Ÿ\n])'

        iterator = generate_answer_stream(question_text, history=chat_history)

        for chunk_text in iterator:
            text_buffer += chunk_text
            full_answer_log += chunk_text 

            # [SILENCE] ãƒã‚§ãƒƒã‚¯
            if full_answer_log.strip() == "[SILENCE]":
                logger.info("[TASK] SILENCEæ¤œå‡ºã€‚å¿œç­”ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                await websocket.send_json({"status": "ignored", "message": "ï¼ˆéŸ³å£°ã‚’ç„¡è¦–ã—ã¾ã—ãŸï¼‰"})
                return

            # ãƒãƒƒãƒ•ã‚¡åˆ†å‰²
            sentences = re.split(split_pattern, text_buffer)
            if len(sentences) > 1:
                for sent in sentences[:-1]:
                    if sent.strip():
                        sentence_count += 1
                        await process_sentence(sent, original_filename, sentence_count, websocket)
                text_buffer = sentences[-1]

        # æ®‹ã‚Šå‡¦ç†
        if text_buffer.strip():
            if text_buffer.strip() == "[SILENCE]":
                 await websocket.send_json({"status": "ignored", "message": "ï¼ˆéŸ³å£°ã‚’ç„¡è¦–ã—ã¾ã—ãŸï¼‰"})
                 return

            sentence_count += 1
            await process_sentence(text_buffer, original_filename, sentence_count, websocket)
        
        # å±¥æ­´æ›´æ–°
        chat_history.append({"role": "user", "parts": [question_text]})
        chat_history.append({"role": "model", "parts": [full_answer_log]})
        
        await websocket.send_json({"status": "complete", "answer_text": full_answer_log})
        logger.info(f"[TASK END] å®Œäº†. ç¾åœ¨ã®å±¥æ­´æ•°: {len(chat_history)//2}ã‚¿ãƒ¼ãƒ³")

    except Exception as e:
        logger.error(f"[TASK ERROR] ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        try:
            await websocket.send_json({"status": "error", "message": f"ã‚¨ãƒ©ãƒ¼: {e}"})
        except WebSocketDisconnect:
            pass 

# ---------------------------
# 3. WebSocket ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ---------------------------
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    logger.info("[WS] ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶š")
    
    chat_history = []
    
    try:
        while True:
            audio_data = await websocket.receive_bytes()
            audio_io = io.BytesIO(audio_data)
            
            temp_id = f"ws_{int(time.time())}"
            output_wav_filename = f"{temp_id}.wav"
            output_wav_path = os.path.join(PROCESSING_DIR, output_wav_filename)
            
            def convert_audio():
                try:
                    audio = AudioSegment.from_file(audio_io) 
                    audio = audio.set_frame_rate(16000).set_channels(1)
                    audio.export(output_wav_path, format="wav")
                    return True
                except Exception as e:
                    logger.error(f"[WS ERROR] å¤‰æ›å¤±æ•—: {e}")
                    return False

            if not await asyncio.to_thread(convert_audio):
                await websocket.send_json({"status": "error", "message": "éŸ³å£°å½¢å¼ã‚¨ãƒ©ãƒ¼"})
                continue
            
            # å‡¦ç†é–‹å§‹é€šçŸ¥
            await websocket.send_json({"status": "processing", "message": "èªè­˜ä¸­..."})

            asyncio.create_task(process_audio_file(
                output_wav_path, 
                output_wav_filename, 
                websocket,
                chat_history
            ))
            
    except WebSocketDisconnect:
        logger.info("[WS] åˆ‡æ–­")
    except Exception as e:
        logger.error(f"[WS ERROR] : {e}", exc_info=True)
    finally:
        try:
            await websocket.close()
        except:
            pass


# ---------------------------
# 4. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ (ä¿®æ­£ç‰ˆ HTML/JS - Web Audio APIç‰ˆ)
# ---------------------------
@app.get("/", response_class=HTMLResponse)
async def get_root():
    return """
    <!DOCTYPE html>
    <html lang="ja">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device.width, initial-scale=1.0">
        <title>AI Voice Talk (Web Audio API)</title>
        
        <style>
            body { font-family: sans-serif; display: grid; place-items: center; min-height: 90vh; background: #f0f2f5; }
            #container { background: white; padding: 2rem; border-radius: 12px; box-shadow: 0 8px 20px rgba(0,0,0,0.1); text-align: center; width: 90%; max-width: 600px; }
            
            button {
                font-size: 1rem; padding: 0.8rem 1.5rem; border: none; 
                border-radius: 25px; cursor: pointer; margin: 0.5rem; 
                color: white; transition: transform 0.1s, opacity 0.2s;
                font-weight: bold;
            }
            button:active { transform: scale(0.98); }
            button:disabled { background: #ccc !important; cursor: not-allowed; opacity: 0.6; transform: none; }
            
            #startButton { background: #007bff; }
            #stopButton { background: #6c757d; }

            #status { margin-top: 1.5rem; font-size: 1.1rem; color: #333; min-height: 1.5em; font-weight: bold; }
            #vad-status { font-size: 0.9rem; color: #666; height: 1.5em; margin-bottom: 10px;}
            
            #qa-display { 
                margin: 1rem auto 0 auto; text-align: left; width: 100%; 
                border-top: 2px solid #f0f0f0; padding-top: 1rem; 
                max-height: 400px; overflow-y: auto;
            }
            .bubble {
                padding: 10px 15px; border-radius: 15px; margin-bottom: 10px;
                line-height: 1.5; position: relative;
            }
            .user-bubble { background: #e7f5ff; color: #0056b3; margin-left: 20px; border-bottom-right-radius: 2px;}
            .user-bubble::before { content: 'ã‚ãªãŸ'; font-size: 0.7rem; position: absolute; top: -18px; right: 0; color: #999; }
            
            .ai-bubble { background: #f0fff4; color: #155724; margin-right: 20px; border-bottom-left-radius: 2px;}
            .ai-bubble::before { content: 'AI'; font-size: 0.7rem; position: absolute; top: -18px; left: 0; color: #999; }
        </style>
    </head>
    <body>
        <div id="container">
            <h1>AI Voice Talk âš¡</h1>
            <p>ã„ã¤ã§ã‚‚è©±ã—ã‹ã‘ã¦ãã ã•ã„ï¼ˆå‰²ã‚Šè¾¼ã¿å¯èƒ½ï¼‰</p>
            
            <div>
                <button id="startButton">ãƒã‚¤ã‚¯ON</button>
                <button id="stopButton" disabled>ãƒã‚¤ã‚¯OFF</button>
            </div>
            
            <div id="status">æº–å‚™å®Œäº†</div>
            <div id="vad-status">(å¾…æ©Ÿä¸­)</div>
            
            <div id="qa-display"></div>
        </div>

        <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.22.0/dist/ort.wasm.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.29/dist/bundle.min.js"></script>

        <script>
            // --- ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° ---
            const startButton = document.getElementById('startButton');
            const stopButton = document.getElementById('stopButton');
            const statusDiv = document.getElementById('status');
            const vadStatusDiv = document.getElementById('vad-status');
            const qaDisplay = document.getElementById('qa-display');

            let ws;
            let vad; 
            let mediaStream; 
            
            // Web Audio APIç”¨ã®å¤‰æ•°
            let audioCtx = null;
            let currentSource = null; // ç¾åœ¨å†ç”Ÿä¸­ã®ã‚½ãƒ¼ã‚¹
            
            let isSpeaking = false;     
            let audioQueue = [];        
            let isPlaying = false;      
            let ignoreIncomingAudio = false; 

            // --- 1. WebSocketæ¥ç¶š ---
            function connectWebSocket() {
                const wsProtocol = window.location.protocol === 'https:' ? 'wss://' : 'ws://';
                ws = new WebSocket(wsProtocol + window.location.host + '/ws');
                ws.binaryType = 'arraybuffer';

                ws.onopen = () => {
                    console.log('WebSocket æ¥ç¶š');
                    statusDiv.textContent = 'æ¥ç¶šå®Œäº†ã€‚ãƒã‚¤ã‚¯ã‚’ONã«ã—ã¦ãã ã•ã„ã€‚';
                    startButton.disabled = false;
                };

                ws.onmessage = (event) => {
                    if (event.data instanceof ArrayBuffer) {
                        if (ignoreIncomingAudio) return;
                        const audioBlob = new Blob([event.data], { type: 'audio/mp3' });
                        audioQueue.push(audioBlob);
                        processAudioQueue();
                    } else {
                        try {
                            const data = JSON.parse(event.data);
                            handleJsonMessage(data);
                        } catch (e) { console.error(e); }
                    }
                };

                ws.onclose = () => {
                    statusDiv.textContent = 'å†æ¥ç¶šã—ã¦ãã ã•ã„ã€‚';
                    stopVAD(); 
                };
            }

            // --- 2. UIæ“ä½œ ---
            let currentQuestionId = null;
            let currentAnswerId = null;

            function appendBubble(role, text, id) {
                let div = document.getElementById(id);
                if (!div) {
                    div = document.createElement('div');
                    div.id = id;
                    div.className = `bubble ${role === 'user' ? 'user-bubble' : 'ai-bubble'}`;
                    qaDisplay.appendChild(div);
                    qaDisplay.scrollTop = qaDisplay.scrollHeight;
                }
                div.textContent = text;
            }

            function handleJsonMessage(data) {
                if (data.status === 'processing') {
                    statusDiv.textContent = data.message;
                } else if (data.status === 'transcribed') {
                    currentQuestionId = `q-${Date.now()}`;
                    appendBubble('user', data.question_text, currentQuestionId);
                    currentAnswerId = `a-${Date.now()}`;
                    appendBubble('ai', '...', currentAnswerId);
                } else if (data.status === 'reply_chunk') {
                    if (ignoreIncomingAudio) return;
                    const div = document.getElementById(currentAnswerId);
                    if (div) {
                        if (div.textContent === '...') div.textContent = '';
                        div.textContent += data.text_chunk;
                        qaDisplay.scrollTop = qaDisplay.scrollHeight;
                    }
                } else if (data.status === 'ignored') {
                    statusDiv.textContent = "(éŸ³å£°ã‚’ç„¡è¦–ã—ã¾ã—ãŸ)";
                    const div = document.getElementById(currentAnswerId);
                    if(div) div.textContent = "(å¿œç­”ãªã—)";
                } else if (data.status === 'error') {
                    statusDiv.textContent = `ã‚¨ãƒ©ãƒ¼: ${data.message}`;
                }
            }

            // --- 3. Web Audio API åˆæœŸåŒ– (æœ€é‡è¦) ---
            // ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯æ™‚ã«1å›ã ã‘å‘¼ã³å‡ºã—ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã€Œå†é–‹(resume)ã€çŠ¶æ…‹ã«ã™ã‚‹
            async function initAudioContext() {
                if (!audioCtx) {
                    audioCtx = new (window.AudioContext || window.webkitAudioContext)();
                }
                
                // ãƒ–ãƒ©ã‚¦ã‚¶ã«ã‚ˆã£ã¦ã‚µã‚¹ãƒšãƒ³ãƒ‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å†é–‹ã•ã›ã‚‹
                if (audioCtx.state === 'suspended') {
                    await audioCtx.resume();
                }

                // ç„¡éŸ³ã‚’å†ç”Ÿã—ã¦ç¢ºå®Ÿã«ãƒ­ãƒƒã‚¯è§£é™¤ã™ã‚‹
                const buffer = audioCtx.createBuffer(1, 1, 22050);
                const source = audioCtx.createBufferSource();
                source.buffer = buffer;
                source.connect(audioCtx.destination);
                source.start(0);
                
                console.log("ğŸ”Š AudioContext unlocked/resumed:", audioCtx.state);
            }

            // --- 4. éŸ³å£°å†ç”Ÿãƒ­ã‚¸ãƒƒã‚¯ (Web Audio APIç‰ˆ) ---
            function processAudioQueue() {
                if (isPlaying) return;
                if (audioQueue.length === 0) return;
                
                const nextBlob = audioQueue.shift();
                playAudioBlob(nextBlob);
            }

            async function playAudioBlob(blob) {
                if (!audioCtx) return; // åˆæœŸåŒ–å‰ãªã‚‰ç„¡è¦–

                isPlaying = true;
                statusDiv.textContent = 'ğŸ”Š AIå›ç­”ä¸­...';

                try {
                    // Blob -> ArrayBuffer -> AudioBuffer
                    const arrayBuffer = await blob.arrayBuffer();
                    const audioBuffer = await audioCtx.decodeAudioData(arrayBuffer);

                    const source = audioCtx.createBufferSource();
                    source.buffer = audioBuffer;
                    source.connect(audioCtx.destination);
                    
                    currentSource = source; // ä¸­æ–­ã§ãã‚‹ã‚ˆã†ã«ä¿å­˜

                    source.onended = () => {
                        isPlaying = false;
                        currentSource = null;
                        processAudioQueue();
                        
                        if (audioQueue.length === 0) {
                            statusDiv.textContent = 'ğŸŸ¢ å®Œäº†ã€‚æ¬¡ã®è³ªå•ã‚’ã©ã†ãã€‚';
                        }
                    };

                    source.start(0);
                    
                } catch (e) {
                    console.error("å†ç”Ÿã‚¨ãƒ©ãƒ¼(decode/play):", e);
                    isPlaying = false;
                    processAudioQueue();
                }
            }

            // --- 5. å‰²ã‚Šè¾¼ã¿å‡¦ç† ---
            function interruptAudio() {
                // å†ç”Ÿä¸­ã®ã‚½ãƒ¼ã‚¹ã‚’åœæ­¢
                if (currentSource) {
                    try { currentSource.stop(); } catch(e){}
                    currentSource = null;
                }
                
                audioQueue = [];
                isPlaying = false;
                ignoreIncomingAudio = true;
                statusDiv.textContent = 'â›” ä¸­æ–­ã€‚ã‚ãªãŸã®å£°ã‚’èã„ã¦ã„ã¾ã™ã€‚';
                
                if (currentAnswerId) {
                    const div = document.getElementById(currentAnswerId);
                    if (div) div.textContent += " (ä¸­æ–­)";
                }
            }

            // --- 6. VAD & ãƒã‚¤ã‚¯è¨­å®š ---
            async function setupVAD() {
                try {
                    startButton.disabled = true;
                    statusDiv.textContent = 'ãƒã‚¤ã‚¯æº–å‚™ä¸­...';

                    while (!window.vad) await new Promise(r => setTimeout(r, 50));
                    
                    mediaStream = await navigator.mediaDevices.getUserMedia({ 
                        audio: {
                            echoCancellation: true,
                            noiseSuppression: true,
                            autoGainControl: true
                        } 
                    });
                    
                    vad = await window.vad.MicVAD.new({
                        stream: mediaStream,
                        positiveSpeechThreshold: 0.9, // èª¤æ¤œçŸ¥é˜²æ­¢ã§å°‘ã—é«˜ã‚
                        minSpeechFrames: 4,
                        preSpeechPadFrames: 20,
                        onnxWASMBasePath: "https://cdn.jsdelivr.net/npm/onnxruntime-web@1.22.0/dist/",
                        baseAssetPath: "https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.29/dist/",
                        
                        onSpeechStart: () => {
                            isSpeaking = true;
                            vadStatusDiv.textContent = "ğŸ—£ï¸ æ„ŸçŸ¥ä¸­...";
                            if (isPlaying || audioQueue.length > 0) {
                                interruptAudio(); // å‰²ã‚Šè¾¼ã¿
                            }
                        },
                        
                        onSpeechEnd: (audio) => {
                            isSpeaking = false;
                            vadStatusDiv.textContent = "ğŸ“¡ é€ä¿¡ä¸­...";
                            if (ws && ws.readyState === WebSocket.OPEN) {
                                ignoreIncomingAudio = false; 
                                sendAudioAsWav(audio);
                                statusDiv.textContent = 'AIæ€è€ƒä¸­...';
                            }
                        }
                    });

                    vad.start();
                    stopButton.disabled = false;
                    statusDiv.textContent = 'ğŸŸ¢ æº–å‚™å®Œäº†ã€‚';
                    vadStatusDiv.textContent = 'ğŸ‘‚ å¾…æ©Ÿä¸­';

                } catch (err) {
                    console.error('VAD/Mic ã‚¨ãƒ©ãƒ¼:', err);
                    statusDiv.textContent = 'ãƒã‚¤ã‚¯åˆæœŸåŒ–å¤±æ•—ã€‚';
                    startButton.disabled = false;
                }
            }

            // --- ãã®ä»– ---
            function sendAudioAsWav(float32Array) {
                const wavBuffer = encodeWAV(float32Array, 16000); 
                ws.send(wavBuffer);
            }
            function stopVAD() {
                vad?.destroy(); 
                vad = null;
                mediaStream?.getTracks().forEach(track => track.stop());
                startButton.disabled = false;
                stopButton.disabled = true;
                statusDiv.textContent = 'åœæ­¢ä¸­';
            }
            function encodeWAV(samples, sampleRate) {
                const buffer = new ArrayBuffer(44 + samples.length * 2);
                const view = new DataView(buffer);
                writeString(view, 0, 'RIFF');
                view.setUint32(4, 36 + samples.length * 2, true);
                writeString(view, 8, 'WAVE');
                writeString(view, 12, 'fmt ');
                view.setUint32(16, 16, true);
                view.setUint16(20, 1, true); 
                view.setUint16(22, 1, true); 
                view.setUint32(24, sampleRate, true);
                view.setUint32(28, sampleRate * 2, true);
                view.setUint16(32, 2, true);
                view.setUint16(34, 16, true);
                writeString(view, 36, 'data');
                view.setUint32(40, samples.length * 2, true);
                floatTo16BitPCM(view, 44, samples);
                return view;
            }
            function writeString(view, offset, string) {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            }
            function floatTo16BitPCM(output, offset, input) {
                for (let i = 0; i < input.length; i++, offset += 2) {
                    let s = Math.max(-1, Math.min(1, input[i]));
                    s = s < 0 ? s * 0x8000 : s * 0x7FFF;
                    output.setInt16(offset, s, true);
                }
            }

            // â˜…ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯ã§AudioContextåˆæœŸåŒ–ã¨VADèµ·å‹•ã‚’åŒæ™‚ã«è¡Œã†
            startButton.onclick = async () => {
                await initAudioContext(); // ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¨ãƒ³ã‚¸ãƒ³ã®èµ·å‹•
                await setupVAD();         // ãƒã‚¤ã‚¯ã®èµ·å‹•
            };
            
            stopButton.onclick = stopVAD;
            window.onload = connectWebSocket;
        </script>
    </body>
    </html>
    """

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    logger.info(f"ã‚µãƒ¼ãƒãƒ¼ã‚’ http://0.0.0.0:{port} ã§èµ·å‹•ã—ã¾ã™ã€‚")
    uvicorn.run(app, host="0.0.0.0", port=port, log_config=None)