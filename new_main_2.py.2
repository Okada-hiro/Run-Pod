# /workspace/new_new_main.py
# Server-Side VAD (Silero) + Streaming Architecture

import uvicorn
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
import torch
import numpy as np
import asyncio
import logging
import sys
import os
import io
import re

# --- ãƒ­ã‚®ãƒ³ã‚°è¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# --- å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
try:
    # æ—¢å­˜ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’åˆ©ç”¨
    from transcribe_func import GLOBAL_ASR_MODEL_INSTANCE
    from supporter_generator import generate_answer_stream
    from new_text_to_speech import synthesize_speech
    from new_speaker_filter import SpeakerGuard
except ImportError as e:
    logger.error(f"[ERROR] å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    sys.exit(1)

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š ---
PROCESSING_DIR = "incoming_audio"
os.makedirs(PROCESSING_DIR, exist_ok=True)

# L4 GPU ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã®è¨­å®š
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"Using Device for VAD: {DEVICE}")

app = FastAPI()
app.mount(f"/download", StaticFiles(directory=PROCESSING_DIR), name="download")

speaker_guard = SpeakerGuard()
NEXT_AUDIO_IS_REGISTRATION = False

# --- â˜… Silero VAD ã®ãƒ­ãƒ¼ãƒ‰ (ã‚µãƒ¼ãƒãƒ¼ã‚µã‚¤ãƒ‰VAD) ---
logger.info("â³ Loading Silero VAD model...")
try:
    # GitHubã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ (åˆå›ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒç™ºç”Ÿã—ã¾ã™)
    vad_model, utils = torch.hub.load(
        repo_or_dir='snakers4/silero-vad',
        model='silero_vad',
        force_reload=False,
        onnx=False
    )
    (get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils
    
    # GPUã¸è»¢é€ (L4æ´»ç”¨)
    vad_model.to(DEVICE)
    logger.info("âœ… Silero VAD model loaded successfully.")
except Exception as e:
    logger.critical(f"Silero VAD Load Failed: {e}")
    sys.exit(1)

# --- ç™»éŒ²ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ ---
@app.post("/enable-registration")
async def enable_registration():
    global NEXT_AUDIO_IS_REGISTRATION
    NEXT_AUDIO_IS_REGISTRATION = True
    logger.info("ã€ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ã€‘æ¬¡ã®ç™ºè©±ã‚’æ–°è¦è©±è€…ã¨ã—ã¦ç™»éŒ²ã—ã¾ã™")
    return {"message": "ç™»éŒ²ãƒ¢ãƒ¼ãƒ‰å¾…æ©Ÿä¸­"}

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼: éŸ³å£°å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ---
async def process_voice_pipeline(audio_float32_np, websocket: WebSocket, chat_history: list):
    global NEXT_AUDIO_IS_REGISTRATION
    
    # 1. è©±è€…èªè­˜ (SpeakerGuard)
    # SpeakerGuardã¯ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¾ãŸã¯Tensorã‚’æœŸå¾…ã™ã‚‹ãŸã‚ã€ä¸€æ™‚ä¿å­˜ã›ãšã«Tensorã§æ¸¡ã™ã‚ˆã†æ”¹é€ ã™ã‚‹ã‹ã€
    # ã“ã“ã§ã¯äº’æ›æ€§ç¶­æŒã®ãŸã‚ä¸€æ™‚ãƒãƒƒãƒ•ã‚¡ã‚’ä½¿ã„ã¾ã™ï¼ˆæœ¬æ¥ã¯ç›´æ¥æ¸¡ã™ã¹ãã§ã™ãŒå®‰å…¨ç­–ã‚’ã¨ã‚Šã¾ã™ï¼‰
    
    # numpy -> torch tensor
    audio_tensor = torch.from_numpy(audio_float32_np).float()
    
    # ç™»éŒ²ãƒ¢ãƒ¼ãƒ‰
    if NEXT_AUDIO_IS_REGISTRATION:
        # SpeakerGuardãŒãƒ‘ã‚¹å¿…é ˆãªã‚‰ä¸€æ™‚ä¿å­˜ã€æ”¹é€ æ¸ˆã¿ãªã‚‰Tensoræ¸¡ã—
        # ã“ã“ã§ã¯æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨ã®äº’æ›æ€§ã®ãŸã‚ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã™
        temp_reg_path = f"{PROCESSING_DIR}/reg_{id(audio_float32_np)}.wav"
        import soundfile as sf
        sf.write(temp_reg_path, audio_float32_np, 16000)
        
        success = await asyncio.to_thread(speaker_guard.register_new_speaker, temp_reg_path)
        NEXT_AUDIO_IS_REGISTRATION = False
        if success:
            await websocket.send_json({"status": "ignored", "message": "âœ… ãƒ¡ãƒ³ãƒãƒ¼ç™»éŒ²å®Œäº†ï¼"})
        else:
            await websocket.send_json({"status": "error", "message": "ç™»éŒ²å¤±æ•—"})
        return

    # æœ¬äººç¢ºèª (ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«çµŒç”±å›é¿ã®ãŸã‚ã€embeddingæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ã«ä¾å­˜ã™ã‚‹ãŒã€
    # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ã€Œå¸¸ã«OKã€ã¾ãŸã¯ã€ŒSpeakerGuardã®æ”¹é€ ã€ãŒå¿…è¦ã€‚
    # ã„ã£ãŸã‚“ã‚¹ã‚­ãƒƒãƒ—ã›ãšã€æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã‚’é€šã™ãŸã‚ã«ãƒ¡ãƒ¢ãƒªä¸Šã®å‡¦ç†ã‚’æ¨å¥¨)
    # â€»ä»Šå›ã¯é«˜é€ŸåŒ–å„ªå…ˆã®ãŸã‚ã€ã‚¬ãƒ¼ãƒ‰åˆ¤å®šã‚’ãƒ‘ã‚¹ã—ã¦ç›´Whisperã«è¡Œãã¾ã™ãŒã€
    #   å¿…è¦ãªã‚‰ã“ã“ã« is_owner ãƒ­ã‚¸ãƒƒã‚¯ã‚’æŒŸã‚“ã§ãã ã•ã„ã€‚
    
    # 2. Whisper æ–‡å­—èµ·ã“ã— (ãƒ¡ãƒ¢ãƒªã‹ã‚‰ç›´æ¥)
    try:
        if GLOBAL_ASR_MODEL_INSTANCE is None:
            raise ValueError("Whisper Model not loaded")

        logger.info("[TASK] æ–‡å­—èµ·ã“ã—é–‹å§‹ (Memory)")
        
        # faster-whisper ã¯ numpy array (float32) ã‚’ç›´æ¥å—ã‘å–ã‚Œã¾ã™
        segments = await asyncio.to_thread(
            GLOBAL_ASR_MODEL_INSTANCE.transcribe, 
            audio_float32_np
        )
        
        # ãƒ†ã‚­ã‚¹ãƒˆçµåˆ
        text = "".join([s[2] for s in GLOBAL_ASR_MODEL_INSTANCE.ts_words(segments)])
        
        if not text.strip():
            logger.info("[TASK] éŸ³å£°èªè­˜çµæœãŒç©ºã§ã—ãŸ")
            await websocket.send_json({"status": "ignored", "message": "..."})
            return

        logger.info(f"[TASK] èªè­˜ãƒ†ã‚­ã‚¹ãƒˆ: {text}")
        await websocket.send_json({
            "status": "transcribed",
            "question_text": text
        })

        # 3. LLM & TTS ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
        await handle_llm_tts(text, websocket, chat_history)

    except Exception as e:
        logger.error(f"Pipeline Error: {e}", exc_info=True)
        await websocket.send_json({"status": "error", "message": "å‡¦ç†ã‚¨ãƒ©ãƒ¼"})

async def handle_llm_tts(text: str, websocket: WebSocket, chat_history: list):
    """å›ç­”ç”Ÿæˆã¨éŸ³å£°åˆæˆã®ä¸¦åˆ—å‡¦ç†"""
    text_buffer = ""
    sentence_count = 0
    full_answer = ""
    # â˜…ä¿®æ­£: ã€Œã€ã€ã‚‚å«ã‚ã¦ç´°ã‹ãåŒºåˆ‡ã‚‹ã“ã¨ã§ä½“æ„Ÿé€Ÿåº¦ã‚¢ãƒƒãƒ—
    split_pattern = r'(?<=[ã€‚ï¼ï¼Ÿ\nã€])'

    iterator = generate_answer_stream(text, history=chat_history)

    async def send_audio_chunk(phrase, idx):
        filename = f"resp_{idx}.wav"
        path = os.path.join(PROCESSING_DIR, filename)
        # åˆæˆ
        success = await asyncio.to_thread(
            synthesize_speech, phrase, path
        )
        if success:
            with open(path, 'rb') as f:
                wav_data = f.read()
            await websocket.send_bytes(wav_data)

    try:
        for chunk in iterator:
            text_buffer += chunk
            full_answer += chunk
            
            # SILENCEåˆ¤å®š
            if full_answer.strip() == "[SILENCE]":
                await websocket.send_json({"status": "ignored", "message": "ï¼ˆå¿œç­”ãªã—ï¼‰"})
                return

            sentences = re.split(split_pattern, text_buffer)
            if len(sentences) > 1:
                for sent in sentences[:-1]:
                    if sent.strip():
                        sentence_count += 1
                        # å¥èª­ç‚¹é€ä¿¡ï¼ˆå­—å¹•ç”¨ï¼‰
                        await websocket.send_json({"status": "reply_chunk", "text_chunk": sent})
                        # éŸ³å£°åˆæˆ & é€ä¿¡
                        await send_audio_chunk(sent, sentence_count)
                text_buffer = sentences[-1]
        
        # æ®‹ã‚Š
        if text_buffer.strip():
            sentence_count += 1
            await websocket.send_json({"status": "reply_chunk", "text_chunk": text_buffer})
            await send_audio_chunk(text_buffer, sentence_count)

        # å±¥æ­´æ›´æ–°
        chat_history.append({"role": "user", "parts": [text]})
        chat_history.append({"role": "model", "parts": [full_answer]})
        
        await websocket.send_json({"status": "complete", "answer_text": full_answer})

    except Exception as e:
        logger.error(f"LLM/TTS Error: {e}")


# --- WebSocket ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ (ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°VADå®Ÿè£…) ---
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    logger.info("[WS] Client Connected. Starting VAD Stream.")
    
    vad_iterator = VADIterator(vad_model)
    audio_buffer = [] 
    is_speaking = False
    
    WINDOW_SIZE_SAMPLES = 512 
    SAMPLE_RATE = 16000
    chat_history = []

    try:
        while True:
            data_bytes = await websocket.receive_bytes()
            audio_chunk_np = np.frombuffer(data_bytes, dtype=np.float32).copy()
            
            offset = 0
            while offset + WINDOW_SIZE_SAMPLES <= len(audio_chunk_np):
                window_np = audio_chunk_np[offset : offset + WINDOW_SIZE_SAMPLES]
                offset += WINDOW_SIZE_SAMPLES
                
                window_tensor = torch.from_numpy(window_np).unsqueeze(0).to(DEVICE)

                # --- VAD åˆ¤å®š ---
                speech_dict = await asyncio.to_thread(vad_iterator, window_tensor, return_seconds=True)
                
                # â˜…ä¿®æ­£: speech_dict ãŒ None ã§ãªã„ã‹ç¢ºèªã™ã‚‹
                if speech_dict:
                    if "start" in speech_dict:
                        logger.info("ğŸ—£ï¸ [VAD] Speech STARTED")
                        is_speaking = True
                        await websocket.send_json({"status": "processing", "message": "èã„ã¦ã„ã¾ã™..."})
                        audio_buffer = [window_np] 
                    
                    elif "end" in speech_dict:
                        logger.info("ğŸ¤« [VAD] Speech ENDED")
                        if is_speaking:
                            is_speaking = False
                            audio_buffer.append(window_np)
                            
                            full_audio = np.concatenate(audio_buffer)
                            if len(full_audio) / SAMPLE_RATE < 0.2:
                                logger.info("Noise detected (too short), ignoring.")
                            else:
                                await process_voice_pipeline(full_audio, websocket, chat_history)
                            audio_buffer = [] 
                
                else:
                    # speech_dict ãŒ None (ã‚¤ãƒ™ãƒ³ãƒˆãªã—) ã®å ´åˆ
                    # è©±ã—ã¦ã„ã‚‹æœ€ä¸­ãªã‚‰ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ ã—ç¶šã‘ã‚‹
                    if is_speaking:
                        audio_buffer.append(window_np)

    except WebSocketDisconnect:
        logger.info("[WS] Disconnected")
    except Exception as e:
        logger.error(f"[WS ERROR] {e}", exc_info=True)
    finally:
        vad_iterator.reset_states()


# --- ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ (ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç‰¹åŒ–ç‰ˆ) ---
@app.get("/", response_class=HTMLResponse)
async def get_root():
    return """
    <!DOCTYPE html>
    <html lang="ja">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device.width, initial-scale=1.0">
        <title>Realtime Voice Stream âš¡</title>
        <style>
            body { font-family: sans-serif; display: grid; place-items: center; min-height: 90vh; background: #222; color: #fff; }
            #container { background: #333; padding: 2rem; border-radius: 12px; text-align: center; width: 90%; max-width: 600px; }
            button { padding: 1rem 2rem; border-radius: 30px; border: none; font-size: 1.2rem; cursor: pointer; margin: 10px; font-weight: bold;}
            #btn-start { background: #00d2ff; color: #000; }
            #btn-stop { background: #ff4b4b; color: #fff; display: none; }
            #status { margin-top: 1rem; font-size: 1.2rem; min-height: 1.5em; }
            .bubble { text-align: left; padding: 10px; margin: 5px; border-radius: 10px; background: #444; }
            .ai { background: #005c4b; color: #fff; margin-right: 20px; }
            .user { background: #202c33; color: #ccc; margin-left: 20px; }
            #chat-box { height: 300px; overflow-y: auto; margin-top: 20px; border: 1px solid #555; padding: 10px; }
        </style>
    </head>
    <body>
        <div id="container">
            <h1>Realtime Talk (L4 GPU)</h1>
            <button id="btn-start">ä¼šè©±ã‚’å§‹ã‚ã‚‹</button>
            <button id="btn-stop">åœæ­¢</button>
            <div id="status">å¾…æ©Ÿä¸­</div>
            <div id="chat-box"></div>
        </div>

        <script>
            let socket;
            let audioContext;
            let processor;
            let source;
            let isRecording = false;
            
            const btnStart = document.getElementById('btn-start');
            const btnStop = document.getElementById('btn-stop');
            const statusDiv = document.getElementById('status');
            const chatBox = document.getElementById('chat-box');

            // éŸ³å£°å†ç”Ÿç”¨ã‚­ãƒ¥ãƒ¼
            let audioQueue = [];
            let isPlaying = false;

            // --- UIæ“ä½œ ---
            function logChat(role, text) {
                const div = document.createElement('div');
                div.className = `bubble ${role}`;
                div.textContent = text;
                chatBox.appendChild(div);
                chatBox.scrollTop = chatBox.scrollHeight;
            }

            // --- WebSocket & Audio ---
            async function startRecording() {
                try {
                    statusDiv.textContent = "æ¥ç¶šä¸­...";
                    const wsProtocol = window.location.protocol === 'https:' ? 'wss://' : 'ws://';
                    socket = new WebSocket(wsProtocol + window.location.host + '/ws');
                    socket.binaryType = 'arraybuffer';

                    socket.onopen = async () => {
                        console.log("WS Connected");
                        statusDiv.textContent = "ğŸ™ï¸ ãŠè©±ã—ãã ã•ã„ (Server VAD)";
                        btnStart.style.display = 'none';
                        btnStop.style.display = 'inline-block';
                        await initAudioStream();
                    };

                    socket.onmessage = async (event) => {
                        if (event.data instanceof ArrayBuffer) {
                            // éŸ³å£°å—ä¿¡ -> å†ç”Ÿã‚­ãƒ¥ãƒ¼ã¸
                            audioQueue.push(event.data);
                            processAudioQueue();
                        } else {
                            const data = JSON.parse(event.data);
                            if (data.status === 'processing') statusDiv.textContent = data.message;
                            if (data.status === 'transcribed') logChat('user', data.question_text);
                            if (data.status === 'complete') logChat('ai', data.answer_text);
                            if (data.status === 'reply_chunk') {
                                // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤ºãŒå¿…è¦ãªã‚‰ã“ã“ã«
                            }
                        }
                    };

                    socket.onclose = () => stopRecording();

                } catch (e) {
                    console.error(e);
                    statusDiv.textContent = "ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ";
                }
            }

            async function initAudioStream() {
                // Silero VAD ã¯ 16000Hz ãŒç†æƒ³
                audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
                
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: { 
                        channelCount: 1, 
                        echoCancellation: true, 
                        noiseSuppression: true,
                        autoGainControl: true
                    } 
                });
                
                source = audioContext.createMediaStreamSource(stream);
                
                // Processorä½œæˆ (ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º 4096)
                // AudioWorkletãŒãƒ™ã‚¹ãƒˆã§ã™ãŒã€ç°¡ä¾¿ã®ãŸã‚ScriptProcessorã‚’ä½¿ç”¨
                processor = audioContext.createScriptProcessor(4096, 1, 1);
                
                processor.onaudioprocess = (e) => {
                    if (!socket || socket.readyState !== WebSocket.OPEN) return;
                    
                    const inputData = e.inputBuffer.getChannelData(0);
                    // Float32Arrayã‚’ãã®ã¾ã¾é€ã‚‹ (ã‚µãƒ¼ãƒãƒ¼å´ã§numpyå¤‰æ›)
                    socket.send(inputData.buffer);
                };
                
                source.connect(processor);
                processor.connect(audioContext.destination); // éŒ²éŸ³ã‚’æœ‰åŠ¹ã«ã™ã‚‹ãŸã‚æ¥ç¶šãŒå¿…è¦ï¼ˆãƒŸãƒ¥ãƒ¼ãƒˆæ¨å¥¨ã ãŒä»Šå›ã¯ç°¡ç•¥åŒ–ï¼‰
                isRecording = true;
            }

            function stopRecording() {
                isRecording = false;
                if (source) source.disconnect();
                if (processor) processor.disconnect();
                if (audioContext) audioContext.close();
                if (socket) socket.close();
                
                btnStart.style.display = 'inline-block';
                btnStop.style.display = 'none';
                statusDiv.textContent = "åœæ­¢ä¸­";
            }

            // --- å†ç”Ÿãƒ­ã‚¸ãƒƒã‚¯ ---
            async function processAudioQueue() {
                if (isPlaying || audioQueue.length === 0) return;
                isPlaying = true;
                const wavData = audioQueue.shift();
                
                try {
                    // ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰é€ã‚‰ã‚Œã¦ãã‚‹WAVã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦å†ç”Ÿ
                    // (å†ç”Ÿç”¨AudioContextã¯åˆ¥é€”ä½œã‚‹ã‹ã€æ—¢å­˜ã®ã‚‚ã®ã‚’ä½¿ã†)
                    if (!audioContext || audioContext.state === 'closed') {
                         audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    }
                    const audioBuffer = await audioContext.decodeAudioData(wavData);
                    const source = audioContext.createBufferSource();
                    source.buffer = audioBuffer;
                    source.connect(audioContext.destination);
                    source.onended = () => {
                        isPlaying = false;
                        processAudioQueue();
                    };
                    source.start(0);
                } catch(e) {
                    console.error("å†ç”Ÿã‚¨ãƒ©ãƒ¼", e);
                    isPlaying = false;
                }
            }

            btnStart.onclick = startRecording;
            btnStop.onclick = stopRecording;
        </script>
    </body>
    </html>
    """

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)