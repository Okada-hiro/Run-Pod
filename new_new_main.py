# /workspace/new_new_main.py (ä¿®æ­£ç‰ˆ: ãƒãƒ¼ã‚¸ã‚¤ãƒ³å¯¾å¿œ)
import uvicorn
from fastapi import FastAPI, WebSocket, Request
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.websockets import WebSocketDisconnect
import os
import asyncio
import time
import logging 
import sys 
from pydub import AudioSegment
import io
import re

# --- ãƒ­ã‚®ãƒ³ã‚°è¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)] 
)
logger = logging.getLogger(__name__)

# --- å‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
try:
    from transcribe_func import whisper_text_only
    # supporter_generator ã‚’å„ªå…ˆã—ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    try:
        from supporter_generator import generate_answer_stream
    except ImportError:
        from new_answer_generator import generate_answer_stream

    from new_text_to_speech import synthesize_speech
except ImportError as e:
    print(f"[ERROR] å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")

# --- è¨­å®š ---
PROCESSING_DIR = "incoming_audio" 
LANGUAGE = "ja"

# --- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ– ---
app = FastAPI()
os.makedirs(PROCESSING_DIR, exist_ok=True)
app.mount(f"/download", StaticFiles(directory=PROCESSING_DIR), name="download")


# ---------------------------
# 1. æ–‡ã”ã¨ã®å‡¦ç†é–¢æ•°
# ---------------------------
async def process_sentence(text: str, base_filename: str, index: int, websocket: WebSocket):
    logger.info(f"[STREAM] æ–‡{index}: {text[:20]}...")
    
    # (A) å­—å¹•é€ä¿¡
    try:
        await websocket.send_json({
            "status": "reply_chunk",
            "text_chunk": text
        })
    except Exception as e:
        logger.error(f"[STREAM ERROR] ãƒ†ã‚­ã‚¹ãƒˆé€ä¿¡å¤±æ•—: {e}")

    # (B) éŸ³å£°åˆæˆ
    part_filename = f"{base_filename}.part{index}.wav"
    part_path_abs = os.path.abspath(os.path.join(PROCESSING_DIR, part_filename))

    success = await asyncio.to_thread(
        synthesize_speech,
        text_to_speak=text,
        output_wav_path=part_path_abs
    )
    
    if success:
        try:
            # (C) WAV -> MP3
            audio_segment = AudioSegment.from_wav(part_path_abs)
            mp3_buffer = io.BytesIO()
            audio_segment.export(mp3_buffer, format="mp3", bitrate="128k")
            audio_data = mp3_buffer.getvalue()

            # (D) é€ä¿¡
            await websocket.send_bytes(audio_data)
        except Exception as e:
            logger.error(f"[STREAM ERROR] éŸ³å£°å¤‰æ›ãƒ»é€ä¿¡ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)


# ---------------------------
# 2. ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç† (ãƒ¡ã‚¤ãƒ³ãƒ•ãƒ­ãƒ¼)
# ---------------------------
async def process_audio_file(audio_path: str, original_filename: str, websocket: WebSocket, chat_history: list):
    logger.info(f"[TASK START] ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {original_filename}")
    
    try:
        # --- æ–‡å­—èµ·ã“ã— ---
        output_txt_path = os.path.join(PROCESSING_DIR, original_filename + ".txt")
        
        question_text = await asyncio.to_thread(
            whisper_text_only,
            audio_path, language=LANGUAGE, output_txt=output_txt_path
        )
        logger.info(f"[TASK] æ–‡å­—èµ·ã“ã—å®Œäº†: {question_text}")

        await websocket.send_json({
            "status": "transcribed",
            "message": "...",
            "question_text": question_text
        })

        # --- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å›ç­” & ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç† ---
        text_buffer = ""
        sentence_count = 0
        full_answer_log = ""
        split_pattern = r'(?<=[ã€‚ï¼ï¼Ÿ\n])'

        iterator = generate_answer_stream(question_text, history=chat_history)

        for chunk_text in iterator:
            text_buffer += chunk_text
            full_answer_log += chunk_text 

            # [SILENCE] ãƒã‚§ãƒƒã‚¯
            if full_answer_log.strip() == "[SILENCE]":
                logger.info("[TASK] SILENCEæ¤œå‡ºã€‚å¿œç­”ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                await websocket.send_json({"status": "ignored", "message": "ï¼ˆéŸ³å£°ã‚’ç„¡è¦–ã—ã¾ã—ãŸï¼‰"})
                return

            # ãƒãƒƒãƒ•ã‚¡åˆ†å‰²
            sentences = re.split(split_pattern, text_buffer)
            if len(sentences) > 1:
                for sent in sentences[:-1]:
                    if sent.strip():
                        sentence_count += 1
                        await process_sentence(sent, original_filename, sentence_count, websocket)
                text_buffer = sentences[-1]

        # æ®‹ã‚Šå‡¦ç†
        if text_buffer.strip():
            if text_buffer.strip() == "[SILENCE]":
                 await websocket.send_json({"status": "ignored", "message": "ï¼ˆéŸ³å£°ã‚’ç„¡è¦–ã—ã¾ã—ãŸï¼‰"})
                 return

            sentence_count += 1
            await process_sentence(text_buffer, original_filename, sentence_count, websocket)
        
        # å±¥æ­´æ›´æ–°
        chat_history.append({"role": "user", "parts": [question_text]})
        chat_history.append({"role": "model", "parts": [full_answer_log]})
        
        await websocket.send_json({"status": "complete", "answer_text": full_answer_log})
        logger.info(f"[TASK END] å®Œäº†. ç¾åœ¨ã®å±¥æ­´æ•°: {len(chat_history)//2}ã‚¿ãƒ¼ãƒ³")

    except Exception as e:
        logger.error(f"[TASK ERROR] ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        try:
            await websocket.send_json({"status": "error", "message": f"ã‚¨ãƒ©ãƒ¼: {e}"})
        except WebSocketDisconnect:
            pass 

# ---------------------------
# 3. WebSocket ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ---------------------------
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    logger.info("[WS] ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆæ¥ç¶š")
    
    chat_history = []
    
    try:
        while True:
            audio_data = await websocket.receive_bytes()
            audio_io = io.BytesIO(audio_data)
            
            temp_id = f"ws_{int(time.time())}"
            output_wav_filename = f"{temp_id}.wav"
            output_wav_path = os.path.join(PROCESSING_DIR, output_wav_filename)
            
            def convert_audio():
                try:
                    audio = AudioSegment.from_file(audio_io) 
                    audio = audio.set_frame_rate(16000).set_channels(1)
                    audio.export(output_wav_path, format="wav")
                    return True
                except Exception as e:
                    logger.error(f"[WS ERROR] å¤‰æ›å¤±æ•—: {e}")
                    return False

            if not await asyncio.to_thread(convert_audio):
                await websocket.send_json({"status": "error", "message": "éŸ³å£°å½¢å¼ã‚¨ãƒ©ãƒ¼"})
                continue
            
            # å‡¦ç†é–‹å§‹é€šçŸ¥
            await websocket.send_json({"status": "processing", "message": "èªè­˜ä¸­..."})

            asyncio.create_task(process_audio_file(
                output_wav_path, 
                output_wav_filename, 
                websocket,
                chat_history
            ))
            
    except WebSocketDisconnect:
        logger.info("[WS] åˆ‡æ–­")
    except Exception as e:
        logger.error(f"[WS ERROR] : {e}", exc_info=True)
    finally:
        try:
            await websocket.close()
        except:
            pass


# ---------------------------
# 4. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ (ä¿®æ­£ç‰ˆ HTML/JS)
# ---------------------------
@app.get("/", response_class=HTMLResponse)
async def get_root():
    return """
    <!DOCTYPE html>
    <html lang="ja">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device.width, initial-scale=1.0">
        <title>AI Voice Talk (Barge-In)</title>
        
        <style>
            body { font-family: sans-serif; display: grid; place-items: center; min-height: 90vh; background: #f0f2f5; }
            #container { background: white; padding: 2rem; border-radius: 12px; box-shadow: 0 8px 20px rgba(0,0,0,0.1); text-align: center; width: 90%; max-width: 600px; }
            
            button {
                font-size: 1rem; padding: 0.8rem 1.5rem; border: none; 
                border-radius: 25px; cursor: pointer; margin: 0.5rem; 
                color: white; transition: transform 0.1s, opacity 0.2s;
                font-weight: bold;
            }
            button:active { transform: scale(0.98); }
            button:disabled { background: #ccc !important; cursor: not-allowed; opacity: 0.6; transform: none; }
            
            #startButton { background: #007bff; }
            #stopButton { background: #6c757d; }

            #status { margin-top: 1.5rem; font-size: 1.1rem; color: #333; min-height: 1.5em; font-weight: bold; }
            #vad-status { font-size: 0.9rem; color: #666; height: 1.5em; margin-bottom: 10px;}
            
            #qa-display { 
                margin: 1rem auto 0 auto; text-align: left; width: 100%; 
                border-top: 2px solid #f0f0f0; padding-top: 1rem; 
                max-height: 400px; overflow-y: auto;
            }
            .bubble {
                padding: 10px 15px; border-radius: 15px; margin-bottom: 10px;
                line-height: 1.5; position: relative;
            }
            .user-bubble { background: #e7f5ff; color: #0056b3; margin-left: 20px; border-bottom-right-radius: 2px;}
            .user-bubble::before { content: 'ã‚ãªãŸ'; font-size: 0.7rem; position: absolute; top: -18px; right: 0; color: #999; }
            
            .ai-bubble { background: #f0fff4; color: #155724; margin-right: 20px; border-bottom-left-radius: 2px;}
            .ai-bubble::before { content: 'AI'; font-size: 0.7rem; position: absolute; top: -18px; left: 0; color: #999; }

            #audioPlayback { margin-top: 1rem; display: none; }
        </style>
    </head>
    <body>
        <div id="container">
            <h1>AI Voice Talk âš¡</h1>
            <p>ã„ã¤ã§ã‚‚è©±ã—ã‹ã‘ã¦ãã ã•ã„ï¼ˆå‰²ã‚Šè¾¼ã¿å¯èƒ½ï¼‰</p>
            
            <div>
                <button id="startButton">ãƒã‚¤ã‚¯ON</button>
                <button id="stopButton" disabled>ãƒã‚¤ã‚¯OFF</button>
            </div>
            
            <div id="status">æº–å‚™å®Œäº†</div>
            <div id="vad-status">(å¾…æ©Ÿä¸­)</div>
            
            <div id="qa-display">
                </div>

            <div id="audioPlayback"></div>
        </div>

        <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.22.0/dist/ort.wasm.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.29/dist/bundle.min.js"></script>

        <script>
            // --- DOMè¦ç´  ---
            const startButton = document.getElementById('startButton');
            const stopButton = document.getElementById('stopButton');
            const statusDiv = document.getElementById('status');
            const vadStatusDiv = document.getElementById('vad-status');
            const qaDisplay = document.getElementById('qa-display');
            const audioPlayback = document.getElementById('audioPlayback');

            // --- ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° ---
            let ws;
            let vad; 
            let mediaStream; 
            
            // çŠ¶æ…‹ç®¡ç†ãƒ•ãƒ©ã‚°
            let isSpeaking = false;     // ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè©±ã—ã¦ã„ã‚‹ã‹
            let isAISpeaking = false;   // AIãŒå–‹ã£ã¦ã„ã‚‹ã‹ï¼ˆå†ç”Ÿä¸­ã‹ï¼‰
            
            let audioQueue = [];        // å†ç”Ÿå¾…ã¡ã®éŸ³å£°ã‚­ãƒ¥ãƒ¼
            let isPlaying = false;      // ç¾åœ¨éŸ³å£°ã‚’å†ç”Ÿä¸­ã‹
            let currentAudio = null;    // ç¾åœ¨ã®Audioã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            
            // ãƒãƒ¼ã‚¸ã‚¤ãƒ³åˆ¶å¾¡ç”¨: ã€Œå‰ã®å›ç­”ã€ã®æ®‹å…šã‚’ç„¡è¦–ã™ã‚‹ãŸã‚ã®ãƒ•ãƒ©ã‚°
            let ignoreIncomingAudio = false; 

            // UIæ“ä½œç³»
            function appendBubble(role, text, id) {
                let div = document.getElementById(id);
                if (!div) {
                    div = document.createElement('div');
                    div.id = id;
                    div.className = `bubble ${role === 'user' ? 'user-bubble' : 'ai-bubble'}`;
                    qaDisplay.appendChild(div);
                    qaDisplay.scrollTop = qaDisplay.scrollHeight;
                }
                div.textContent = text;
                return div;
            }

            function connectWebSocket() {
                const wsProtocol = window.location.protocol === 'https:' ? 'wss://' : 'ws://';
                ws = new WebSocket(wsProtocol + window.location.host + '/ws');
                ws.binaryType = 'arraybuffer';

                ws.onopen = () => {
                    console.log('WebSocket æ¥ç¶š');
                    statusDiv.textContent = 'æ¥ç¶šã—ã¾ã—ãŸã€‚ãƒã‚¤ã‚¯ã‚’ONã«ã—ã¦ãã ã•ã„ã€‚';
                    startButton.disabled = false;
                };

                ws.onmessage = (event) => {
                    // (A) éŸ³å£°ãƒ‡ãƒ¼ã‚¿å—ä¿¡
                    if (event.data instanceof ArrayBuffer) {
                        if (ignoreIncomingAudio) {
                            console.log("å‰²ã‚Šè¾¼ã¿æ¸ˆã¿ã®ãŸã‚ã€å¤ã„éŸ³å£°ãƒ‘ã‚±ãƒƒãƒˆã‚’ç ´æ£„");
                            return;
                        }
                        const audioBlob = new Blob([event.data], { type: 'audio/mp3' });
                        audioQueue.push(audioBlob);
                        processAudioQueue();
                    } 
                    // (B) åˆ¶å¾¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å—ä¿¡
                    else {
                        try {
                            const data = JSON.parse(event.data);
                            handleJsonMessage(data);
                        } catch (e) { console.error(e); }
                    }
                };

                ws.onclose = () => {
                    statusDiv.textContent = 'ã‚µãƒ¼ãƒãƒ¼åˆ‡æ–­ã€‚ãƒªãƒ­ãƒ¼ãƒ‰æ¨å¥¨ã€‚';
                    stopVAD(); 
                };
            }

            // JSONãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            let currentQuestionId = null;
            let currentAnswerId = null;

            function handleJsonMessage(data) {
                if (data.status === 'processing') {
                    // æ–°ã—ã„ã‚¿ãƒ¼ãƒ³é–‹å§‹
                    statusDiv.textContent = data.message;
                    
                    // â˜…ã“ã“é‡è¦: æ–°ã—ã„å‡¦ç†ãŒå§‹ã¾ã£ãŸã®ã§ã€ä»¥å‰ã®å‰²ã‚Šè¾¼ã¿ãƒ•ãƒ©ã‚°ã¯è§£é™¤
                    // ãŸã ã—ã€AIãŒå–‹ã£ã¦ã„ã‚‹æœ€ä¸­ãªã‚‰ãã‚Œã¯ã€Œå‰ã®ã‚¿ãƒ¼ãƒ³ã€ãªã®ã§æ­¢ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ãŒ
                    // processingãŒæ¥ã‚‹ï¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè©±ã—çµ‚ã‚ã£ã¦é€ä¿¡ã—ãŸå¾Œãªã®ã§ã€
                    // åŸºæœ¬çš„ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè©±å®Œäº†æ™‚ç‚¹ã§interruptAudioã—ã¦ã‚‹ã¯ãšã€‚
                    
                } else if (data.status === 'transcribed') {
                    currentQuestionId = `q-${Date.now()}`;
                    appendBubble('user', data.question_text, currentQuestionId);
                    
                    currentAnswerId = `a-${Date.now()}`;
                    appendBubble('ai', '...', currentAnswerId);

                } else if (data.status === 'reply_chunk') {
                    if (ignoreIncomingAudio) return; // ç„¡è¦–ãƒ¢ãƒ¼ãƒ‰ãªã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚‚æ›´æ–°ã—ãªã„
                    
                    const div = document.getElementById(currentAnswerId);
                    if (div) {
                        if (div.textContent === '...') div.textContent = '';
                        div.textContent += data.text_chunk;
                        qaDisplay.scrollTop = qaDisplay.scrollHeight;
                    }

                } else if (data.status === 'ignored') {
                    statusDiv.textContent = "ï¼ˆéŸ³å£°ã‚’ç„¡è¦–ã—ã¾ã—ãŸï¼‰";
                    if (currentAnswerId) {
                         const div = document.getElementById(currentAnswerId);
                         if(div) div.textContent = "(å¿œç­”ãªã—)";
                    }

                } else if (data.status === 'error') {
                    statusDiv.textContent = `ã‚¨ãƒ©ãƒ¼: ${data.message}`;
                }
            }

            // --- VAD & ãƒã‚¤ã‚¯è¨­å®š (Barge-Inã®ä¸­æ ¸) ---
            async function setupVAD() {
                try {
                    startButton.disabled = true;
                    statusDiv.textContent = 'VADæº–å‚™ä¸­...';

                    while (!window.vad) await new Promise(r => setTimeout(r, 50));
                    
                    // â˜…é‡è¦1: ã‚¨ã‚³ãƒ¼ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã‚’æœ‰åŠ¹ã«ã™ã‚‹
                    mediaStream = await navigator.mediaDevices.getUserMedia({ 
                        audio: {
                            echoCancellation: true,
                            noiseSuppression: true,
                            autoGainControl: true
                        } 
                    });
                    
                    vad = await window.vad.MicVAD.new({
                        stream: mediaStream,
                        positiveSpeechThreshold: 0.8,
                        minSpeechFrames: 2,
                        preSpeechPadFrames: 20,
                        
                        // â˜…é‡è¦2: è©±ã—å§‹ã‚ã®æ¤œçŸ¥ (å‰²ã‚Šè¾¼ã¿ãƒˆãƒªã‚¬ãƒ¼)
                        onSpeechStart: () => {
                            isSpeaking = true;
                            vadStatusDiv.textContent = "ğŸ—£ï¸ æ„ŸçŸ¥ä¸­...";
                            
                            // ã‚‚ã—AIãŒå–‹ã£ã¦ã„ãŸã‚Šã€å†ç”Ÿå¾…ã¡ãŒã‚ã‚‹å ´åˆã¯ã€Œå‰²ã‚Šè¾¼ã¿ã€ã¨ã¿ãªã™
                            if (isPlaying || audioQueue.length > 0) {
                                console.log("âš¡ å‰²ã‚Šè¾¼ã¿ç™ºç”Ÿï¼ AIã®éŸ³å£°ã‚’åœæ­¢ã—ã¾ã™");
                                interruptAudio();
                            }
                        },
                        
                        // â˜…é‡è¦3: è©±ã—çµ‚ã‚ã‚Šã®æ¤œçŸ¥
                        onSpeechEnd: (audio) => {
                            isSpeaking = false;
                            vadStatusDiv.textContent = "ğŸ“¡ é€ä¿¡ä¸­...";
                            
                            // ã‚µãƒ¼ãƒãƒ¼ã¸é€ä¿¡
                            if (ws && ws.readyState === WebSocket.OPEN) {
                                // æ¬¡ã®AIå›ç­”ã‚’å—ã‘å…¥ã‚Œã‚‹æº–å‚™
                                ignoreIncomingAudio = false; 
                                sendAudioAsWav(audio);
                                statusDiv.textContent = 'AIæ€è€ƒä¸­...';
                            }
                            
                            // â˜…ä»¥å‰ã‚ã£ãŸ vad.pause() ã¯å‰Šé™¤ã€‚å¸¸ã«èãè€³ã‚’ç«‹ã¦ã‚‹ã€‚
                        }
                    });

                    vad.start();
                    stopButton.disabled = false;
                    statusDiv.textContent = 'ğŸŸ¢ æº–å‚™å®Œäº†ã€‚ã„ã¤ã§ã‚‚è©±ã—ã‹ã‘ã¦ãã ã•ã„ã€‚';
                    vadStatusDiv.textContent = 'ğŸ‘‚ å¾…æ©Ÿä¸­';

                } catch (err) {
                    console.error('VADã‚¨ãƒ©ãƒ¼:', err);
                    statusDiv.textContent = 'ãƒã‚¤ã‚¯åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ã€‚';
                    startButton.disabled = false;
                }
            }

            // --- å‰²ã‚Šè¾¼ã¿å‡¦ç†é–¢æ•° ---
            function interruptAudio() {
                // 1. å†ç”Ÿä¸­ã®éŸ³å£°ã‚’æ­¢ã‚ã‚‹
                if (currentAudio) {
                    currentAudio.pause();
                    currentAudio = null;
                }
                
                // 2. å†ç”Ÿå¾…ã¡ã‚­ãƒ¥ãƒ¼ã‚’ç©ºã«ã™ã‚‹
                audioQueue = [];
                isPlaying = false;
                isAISpeaking = false;
                
                // 3. ã“ã‚Œã‹ã‚‰å±Šãã€Œå¤ã„å›ç­”ã®ç¶šãã€ã‚’ç„¡è¦–ã™ã‚‹ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹
                ignoreIncomingAudio = true;
                
                statusDiv.textContent = 'â›” ä¸­æ–­ã—ã¾ã—ãŸã€‚ã‚ãªãŸã®å£°ã‚’èã„ã¦ã„ã¾ã™ã€‚';
                
                // UIä¸Šã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                if (currentAnswerId) {
                    const div = document.getElementById(currentAnswerId);
                    if (div) div.textContent += " (ä¸­æ–­)";
                }
            }

            // --- éŸ³å£°å†ç”Ÿãƒ­ã‚¸ãƒƒã‚¯ ---
            function processAudioQueue() {
                if (isPlaying) return;
                if (audioQueue.length === 0) return;
                
                const nextBlob = audioQueue.shift();
                playAudioBlob(nextBlob);
            }

            function playAudioBlob(blob) {
                isPlaying = true;
                isAISpeaking = true; // AIç™ºè©±ä¸­ãƒ•ãƒ©ã‚°
                statusDiv.textContent = 'ğŸ”Š AIå›ç­”ä¸­...';

                const url = URL.createObjectURL(blob);
                currentAudio = new Audio(url);
                
                currentAudio.onended = () => {
                    isPlaying = false;
                    processAudioQueue(); // æ¬¡ã®æ–‡ã¸
                    
                    // å…¨éƒ¨çµ‚ã‚ã£ãŸã‚‰
                    if (audioQueue.length === 0) {
                        isAISpeaking = false;
                        statusDiv.textContent = 'ğŸŸ¢ å®Œäº†ã€‚æ¬¡ã®è³ªå•ã‚’ã©ã†ãã€‚';
                    }
                };
                
                // ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
                currentAudio.onerror = () => {
                    isPlaying = false;
                    processAudioQueue();
                };

                currentAudio.play().catch(e => {
                    console.error("å†ç”Ÿã‚¨ãƒ©ãƒ¼:", e);
                    isPlaying = false;
                    processAudioQueue();
                });
            }

            // --- ãã®ä»–ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ---
            function sendAudioAsWav(float32Array) {
                const wavBuffer = encodeWAV(float32Array, 16000); 
                ws.send(wavBuffer);
            }

            function stopVAD() {
                vad?.destroy(); 
                vad = null;
                mediaStream?.getTracks().forEach(track => track.stop());
                startButton.disabled = false;
                stopButton.disabled = true;
                statusDiv.textContent = 'åœæ­¢ä¸­';
                vadStatusDiv.textContent = '';
            }

            function encodeWAV(samples, sampleRate) {
                const buffer = new ArrayBuffer(44 + samples.length * 2);
                const view = new DataView(buffer);
                writeString(view, 0, 'RIFF');
                view.setUint32(4, 36 + samples.length * 2, true);
                writeString(view, 8, 'WAVE');
                writeString(view, 12, 'fmt ');
                view.setUint32(16, 16, true);
                view.setUint16(20, 1, true); 
                view.setUint16(22, 1, true); 
                view.setUint32(24, sampleRate, true);
                view.setUint32(28, sampleRate * 2, true);
                view.setUint16(32, 2, true);
                view.setUint16(34, 16, true);
                writeString(view, 36, 'data');
                view.setUint32(40, samples.length * 2, true);
                floatTo16BitPCM(view, 44, samples);
                return view;
            }
            function writeString(view, offset, string) {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            }
            function floatTo16BitPCM(output, offset, input) {
                for (let i = 0; i < input.length; i++, offset += 2) {
                    let s = Math.max(-1, Math.min(1, input[i]));
                    s = s < 0 ? s * 0x8000 : s * 0x7FFF;
                    output.setInt16(offset, s, true);
                }
            }

            startButton.onclick = setupVAD;
            stopButton.onclick = stopVAD;
            window.onload = connectWebSocket;
        </script>
    </body>
    </html>
    """

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    logger.info(f"ã‚µãƒ¼ãƒãƒ¼ã‚’ http://0.0.0.0:{port} ã§èµ·å‹•ã—ã¾ã™ã€‚")
    uvicorn.run(app, host="0.0.0.0", port=port, log_config=None)