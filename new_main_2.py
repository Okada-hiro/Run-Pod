# /workspace/new_main_2.py
# Server-Side VAD + Streaming + Speaker ID + Toast + Filler (å³æ™‚ç›¸æ§Œ)

import uvicorn
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
import torch
import numpy as np
import asyncio
import logging
import sys
import os
import io
import re
import random

# --- ãƒ­ã‚®ãƒ³ã‚°è¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# --- å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
try:
    from transcribe_func import GLOBAL_ASR_MODEL_INSTANCE
    from supporter_generator import generate_answer_stream
    from new_text_to_speech import synthesize_speech
    from new_speaker_filter import SpeakerGuard
except ImportError as e:
    logger.error(f"[ERROR] å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    sys.exit(1)

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š ---
PROCESSING_DIR = "incoming_audio"
FILLER_DIR = "assets_filler" # ç›¸æ§Œç”¨ãƒ•ã‚©ãƒ«ãƒ€
os.makedirs(PROCESSING_DIR, exist_ok=True)
os.makedirs(FILLER_DIR, exist_ok=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"Using Device: {DEVICE}")

app = FastAPI()
app.mount(f"/download", StaticFiles(directory=PROCESSING_DIR), name="download")

# SpeakerGuardåˆæœŸåŒ–
speaker_guard = SpeakerGuard()
NEXT_AUDIO_IS_REGISTRATION = False

# --- â˜…â˜…â˜… ãƒ•ã‚£ãƒ©ãƒ¼ï¼ˆç›¸æ§Œï¼‰éŸ³å£°ã®äº‹å‰ç”Ÿæˆ â˜…â˜…â˜… ---
FILLERS = {
    "filler_short_1": "ã¯ã„ã€",
    "filler_short_2": "ã¯ã„ã€",
    "filler_short_3": "ãã†ã§ã™ã­ã€",
    "filler_ack_1": "æ‰¿çŸ¥ã„ãŸã—ã¾ã—ãŸã€‚",
    "filler_ack_2": "ç¢ºèªã„ãŸã—ã¾ã™ã€‚",
}

def pregenerate_fillers():
    logger.info("â³ Generating filler audios...")
    for name, text in FILLERS.items():
        path = os.path.join(FILLER_DIR, f"{name}.wav")
        if not os.path.exists(path):
            try:
                # TTSã‚’ä½¿ã£ã¦ç”Ÿæˆ
                synthesize_speech(text, path)
                logger.info(f"âœ… Generated filler: {name} ({text})")
            except Exception as e:
                logger.error(f"Failed to generate filler {name}: {e}")
    logger.info("âœ… Filler generation complete.")

# èµ·å‹•æ™‚ã«å®Ÿè¡Œï¼ˆéåŒæœŸã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—å¤–ã§å®Ÿè¡Œã™ã‚‹ãŸã‚ã“ã“ã§å‘¼ã¶ï¼‰
# â€» GLOBAL_TTS_MODELãŒãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã§ã‚ã‚‹ã“ã¨ã‚’å‰æã¨ã™ã‚‹ãŸã‚ã€
#   new_text_to_speech.py ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ™‚ã«ãƒ­ãƒ¼ãƒ‰ãŒèµ°ã‚‹æ§‹æˆãªã‚‰ã“ã‚Œã§OK
try:
    pregenerate_fillers()
except Exception as e:
    logger.warning(f"Filler generation skipped: {e}")


# --- Silero VAD ã®ãƒ­ãƒ¼ãƒ‰ ---
logger.info("â³ Loading Silero VAD model...")
try:
    vad_model, utils = torch.hub.load(
        repo_or_dir='snakers4/silero-vad',
        model='silero_vad',
        force_reload=False,
        onnx=False
    )
    (get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils
    vad_model.to(DEVICE)
    logger.info("âœ… Silero VAD model loaded.")
except Exception as e:
    logger.critical(f"Silero VAD Load Failed: {e}")
    sys.exit(1)


# --- API: ç™»éŒ²ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ ---
@app.post("/enable-registration")
async def enable_registration():
    global NEXT_AUDIO_IS_REGISTRATION
    NEXT_AUDIO_IS_REGISTRATION = True
    logger.info("ã€ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ã€‘æ¬¡ã®ç™ºè©±ã‚’æ–°è¦è©±è€…ã¨ã—ã¦ç™»éŒ²ã—ã¾ã™")
    return {"message": "ç™»éŒ²ãƒ¢ãƒ¼ãƒ‰å¾…æ©Ÿä¸­"}


# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼: éŸ³å£°å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ---
async def process_voice_pipeline(audio_float32_np, websocket: WebSocket, chat_history: list):
    global NEXT_AUDIO_IS_REGISTRATION
    
    # SpeakerGuardç”¨ã« TensoråŒ–
    voice_tensor = torch.from_numpy(audio_float32_np).float().unsqueeze(0)
    
    speaker_id = "Unknown"
    is_allowed = False

    # ---------------------------
    # 1. è©±è€…åˆ¤å®š / ç™»éŒ²ãƒ­ã‚¸ãƒƒã‚¯
    # ---------------------------
    if NEXT_AUDIO_IS_REGISTRATION:
        temp_reg_path = f"{PROCESSING_DIR}/reg_{id(audio_float32_np)}.wav"
        import soundfile as sf
        sf.write(temp_reg_path, audio_float32_np, 16000)
        
        new_id = await asyncio.to_thread(speaker_guard.register_new_speaker, temp_reg_path)
        NEXT_AUDIO_IS_REGISTRATION = False 
        
        if new_id:
            speaker_id = new_id
            is_allowed = True
            await websocket.send_json({"status": "system_info", "message": f"âœ… {new_id} ã‚’ç™»éŒ²ã—ã¾ã—ãŸï¼ä¼šè©±ã‚’ç¶šã‘ã¾ã™ã€‚"})
        else:
            await websocket.send_json({"status": "error", "message": "ç™»éŒ²ã«å¤±æ•—ã—ã¾ã—ãŸ"})
            return
            
    else:
        is_allowed, detected_id = await asyncio.to_thread(speaker_guard.identify_speaker, voice_tensor)
        speaker_id = detected_id

    # ---------------------------
    # 2. ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡
    # ---------------------------
    if not is_allowed:
        logger.info("[Access Denied] ç™»éŒ²ã•ã‚Œã¦ã„ãªã„è©±è€…ã§ã™ã€‚")
        await websocket.send_json({
            "status": "system_alert", 
            "message": "âš ï¸ å¤–éƒ¨ã®ä¼šè©±(æœªç™»éŒ²)ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã—ã¦è¿½åŠ ã™ã‚‹å ´åˆã¯ã€Œãƒ¡ãƒ³ãƒãƒ¼è¿½åŠ ã€ã‹ã‚‰è¡Œã£ã¦ãã ã•ã„ã€‚",
            "alert_type": "unregistered" 
        })
        return

    # ---------------------------
    # 3. Whisper æ–‡å­—èµ·ã“ã—
    # ---------------------------
    try:
        if GLOBAL_ASR_MODEL_INSTANCE is None:
            raise ValueError("Whisper Model not loaded")

        logger.info("[TASK] æ–‡å­—èµ·ã“ã—é–‹å§‹")
        segments = await asyncio.to_thread(
            GLOBAL_ASR_MODEL_INSTANCE.transcribe, 
            audio_float32_np
        )
        
        text = "".join([s[2] for s in GLOBAL_ASR_MODEL_INSTANCE.ts_words(segments)])
        
        if not text.strip():
            logger.info("[TASK] ç©ºã®èªè­˜çµæœ")
            return

        text_with_context = f"ã€{speaker_id}ã€‘ {text}"
        logger.info(f"[TASK] {text_with_context}")
        
        # --- â˜…â˜…â˜… ã“ã“ã§ã€Œç›¸æ§Œã€ã‚’å³æ™‚å†ç”Ÿï¼ â˜…â˜…â˜… ---
        # Geminiã¸ã®å•ã„åˆã‚ã›ä¸­ã«ã€Œé–“ã€ã‚’æŒãŸã›ã‚‹
        # ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•ã‚„æ–‡è„ˆã§å¤‰ãˆã¦ã‚‚è‰¯ã„ãŒã€ã¾ãšã¯ãƒ©ãƒ³ãƒ€ãƒ ã§
        filler_keys = list(FILLERS.keys())
        # å°‘ã—ã ã‘ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’åˆ¶å¾¡ï¼ˆçŸ­ã„è³ªå•ã«ã¯çŸ­ã„ç›¸æ§Œã€ãªã©æœ¬å½“ã¯ã—ãŸæ–¹ãŒã„ã„ï¼‰
        selected_key = random.choice(filler_keys)
        filler_path = os.path.join(FILLER_DIR, f"{selected_key}.wav")

        if os.path.exists(filler_path):
            logger.info(f"âš¡ [Filler] Playing filler: {selected_key}")
            # å­—å¹•ã‚‚å‡ºã™ï¼Ÿ -> å‡ºã™ã¨ã€Œã¯ã„ã€ã€ã¨å‡ºãŸå¾Œã«AIãŒã¾ãŸå–‹ã‚Šã ã™ã®ã§ã€
            # å­—å¹•ã¯å‡ºã•ãšã«éŸ³å£°ã ã‘é€ã‚‹ã®ãŒè‡ªç„¶ï¼ˆã‚ã‚‹ã„ã¯ (...) ãªã©ã‚’å‡ºã™ï¼‰
            # await websocket.send_json({"status": "reply_chunk", "text_chunk": "..."}) 
            
            with open(filler_path, 'rb') as f:
                filler_bytes = f.read()
            
            # éŸ³å£°ã‚’é€ä¿¡
            await websocket.send_bytes(filler_bytes)
        # ---------------------------------------------

        await websocket.send_json({
            "status": "transcribed",
            "question_text": text,
            "speaker_id": speaker_id 
        })

        # ---------------------------
        # 4. LLM & TTS ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
        # ---------------------------
        await handle_llm_tts(text_with_context, websocket, chat_history)

    except Exception as e:
        logger.error(f"Pipeline Error: {e}", exc_info=True)
        await websocket.send_json({"status": "error", "message": "å‡¦ç†ã‚¨ãƒ©ãƒ¼"})


# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼: å›ç­”ç”Ÿæˆã¨éŸ³å£°åˆæˆ ---
async def handle_llm_tts(text_for_llm: str, websocket: WebSocket, chat_history: list):
    text_buffer = ""
    sentence_count = 0
    full_answer = ""
    split_pattern = r'(?<=[ã€‚ï¼ï¼Ÿ\nã€])'

    iterator = generate_answer_stream(text_for_llm, history=chat_history)

    async def send_audio_chunk(phrase, idx):
        filename = f"resp_{idx}.wav"
        path = os.path.join(PROCESSING_DIR, filename)
        success = await asyncio.to_thread(synthesize_speech, phrase, path)
        if success:
            with open(path, 'rb') as f:
                wav_data = f.read()
            try:
                await websocket.send_bytes(wav_data)
            except RuntimeError:
                pass

    try:
        for chunk in iterator:
            text_buffer += chunk
            full_answer += chunk
            
            if full_answer.strip() == "[SILENCE]":
                await websocket.send_json({
                    "status": "system_alert", 
                    "message": "âš ï¸ ä¼šè©±å¤–ã®éŸ³å£°ã¨åˆ¤æ–­ã—ã¾ã—ãŸã€‚ä¼šè©±ã‚’ç¶šã‘ã¦ãã ã•ã„ã€‚",
                    "alert_type": "irrelevant"
                })
                return

            sentences = re.split(split_pattern, text_buffer)
            if len(sentences) > 1:
                for sent in sentences[:-1]:
                    if sent.strip():
                        sentence_count += 1
                        await websocket.send_json({"status": "reply_chunk", "text_chunk": sent})
                        await send_audio_chunk(sent, sentence_count)
                text_buffer = sentences[-1]
        
        if text_buffer.strip():
            sentence_count += 1
            await websocket.send_json({"status": "reply_chunk", "text_chunk": text_buffer})
            await send_audio_chunk(text_buffer, sentence_count)

        chat_history.append({"role": "user", "parts": [text_for_llm]})
        chat_history.append({"role": "model", "parts": [full_answer]})
        
        await websocket.send_json({"status": "complete", "answer_text": full_answer})

    except Exception as e:
        logger.error(f"LLM/TTS Error: {e}")


# ---------------------------
# WebSocket ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ (å¤‰æ›´ãªã—)
# ---------------------------
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    logger.info("[WS] Client Connected.")
    
    vad_iterator = VADIterator(
        vad_model, 
        threshold=0.5, 
        sampling_rate=16000, 
        min_silence_duration_ms=1000, 
        speech_pad_ms=50
    )

    audio_buffer = [] 
    is_speaking = False
    interruption_triggered = False 
    
    WINDOW_SIZE_SAMPLES = 512 
    SAMPLE_RATE = 16000
    CHECK_SPEAKER_SAMPLES = 12000
    
    chat_history = []

    try:
        while True:
            data_bytes = await websocket.receive_bytes()
            audio_chunk_np = np.frombuffer(data_bytes, dtype=np.float32).copy()
            
            offset = 0
            while offset + WINDOW_SIZE_SAMPLES <= len(audio_chunk_np):
                window_np = audio_chunk_np[offset : offset + WINDOW_SIZE_SAMPLES]
                offset += WINDOW_SIZE_SAMPLES
                window_tensor = torch.from_numpy(window_np).unsqueeze(0).to(DEVICE)

                speech_dict = await asyncio.to_thread(vad_iterator, window_tensor, return_seconds=True)
                
                if speech_dict:
                    if "start" in speech_dict:
                        logger.info("ğŸ—£ï¸ Speech START")
                        is_speaking = True
                        interruption_triggered = False 
                        audio_buffer = [window_np]
                        await websocket.send_json({"status": "processing", "message": "ğŸ‘‚ èã„ã¦ã„ã¾ã™..."})
                    
                    elif "end" in speech_dict:
                        logger.info("ğŸ¤« Speech END")
                        if is_speaking:
                            is_speaking = False
                            audio_buffer.append(window_np)
                            full_audio = np.concatenate(audio_buffer)
                            
                            if len(full_audio) / SAMPLE_RATE < 0.2:
                                logger.info("Noise detected")
                                await websocket.send_json({"status": "ignored", "message": "..."})
                            else:
                                await websocket.send_json({"status": "processing", "message": "ğŸ§  AIæ€è€ƒä¸­..."})
                                await process_voice_pipeline(full_audio, websocket, chat_history)
                            audio_buffer = [] 
                else:
                    if is_speaking:
                        audio_buffer.append(window_np)
                        
                        current_len = sum(len(c) for c in audio_buffer)
                        if not interruption_triggered and not NEXT_AUDIO_IS_REGISTRATION and current_len > CHECK_SPEAKER_SAMPLES:
                            temp_audio = np.concatenate(audio_buffer)
                            temp_tensor = torch.from_numpy(temp_audio).float().unsqueeze(0)
                            
                            is_verified, spk_id = await asyncio.to_thread(speaker_guard.identify_speaker, temp_tensor)
                            
                            if is_verified:
                                logger.info(f"âš¡ [Barge-in] {spk_id} ã®å£°ã‚’æ¤œçŸ¥ï¼åœæ­¢æŒ‡ç¤ºã€‚")
                                await websocket.send_json({"status": "interrupt", "message": "ğŸ›‘ éŸ³å£°åœæ­¢"})
                                interruption_triggered = True

    except WebSocketDisconnect:
        logger.info("[WS] Disconnected")
    except Exception as e:
        logger.error(f"[WS ERROR] {e}", exc_info=True)
    finally:
        vad_iterator.reset_states()


# ---------------------------
# ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ (ãã®ã¾ã¾)
# ---------------------------
@app.get("/", response_class=HTMLResponse)
async def get_root():
    # å‰å›ä½œæˆã—ãŸãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ãªã®ã§ã€
    # ãã®ã¾ã¾åˆ©ç”¨ã™ã‚‹ã‹ã€å‰å›ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’ã“ã“ã«è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚
    # ã“ã“ã§ã¯é•·ããªã‚‹ãŸã‚çœç•¥ã›ãšã€å‰å›ã®ã‚³ãƒ¼ãƒ‰ã¨å…¨ãåŒã˜ã‚‚ã®ã‚’è¿”ã™ã¨æƒ³å®šã—ã¦ãã ã•ã„ã€‚
    return """
    <!DOCTYPE html>
    <html lang="ja">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device.width, initial-scale=1.0">
        <title>Team Chat AI</title>
        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; display: grid; place-items: center; min-height: 90vh; background: #202c33; color: #e9edef; margin: 0; }
            #container { background: #111b21; padding: 0; border-radius: 0; text-align: center; width: 100%; max-width: 600px; height: 100vh; display: flex; flex-direction: column; box-shadow: 0 0 20px rgba(0,0,0,0.5); position: relative; overflow: hidden; }
            @media (min-width: 600px) {
                #container { height: 90vh; border-radius: 12px; }
            }
            
            header { background: #202c33; padding: 15px; border-bottom: 1px solid #374045; font-weight: bold; font-size: 1.1rem; display: flex; justify-content: space-between; align-items: center; z-index: 10; }
            
            #chat-box { 
                flex: 1; overflow-y: auto; padding: 20px; 
                background-image: url("https://user-images.githubusercontent.com/15075759/28719144-86dc0f70-73b1-11e7-911d-60d70fcded21.png");
                background-repeat: repeat;
                background-size: 400px;
                background-color: #0b141a;
                position: relative;
            }

            .row { display: flex; width: 100%; margin-bottom: 8px; flex-direction: column; }
            .row.ai { align-items: flex-start; }
            .row.user { align-items: flex-end; }
            .row.system { align-items: center; margin-bottom: 12px; }
            
            .speaker-name { font-size: 0.75rem; color: #8696a0; margin-bottom: 2px; margin-left: 5px; margin-right: 5px;}

            .bubble { 
                padding: 8px 12px; border-radius: 8px; max-width: 75%; 
                font-size: 0.95rem; line-height: 1.4; word-wrap: break-word;
                box-shadow: 0 1px 0.5px rgba(0,0,0,0.13);
            }
            .ai .bubble { background: #202c33; color: #e9edef; border-top-left-radius: 0; }
            
            .user-type-0 .bubble { background: #005c4b; color: #e9edef; border-top-right-radius: 0; }
            .user-type-1 .bubble { background: #0078d4; color: #fff; border-top-right-radius: 0; }
            .user-type-2 .bubble { background: #6b63ff; color: #fff; border-top-right-radius: 0; }
            .user-type-unknown .bubble { background: #374045; color: #e9edef; border-top-right-radius: 0; }
            
            .system-bubble {
                background: #4a3b00;         
                color: #ffecb3;              
                font-size: 0.85rem;
                padding: 6px 16px;
                border-radius: 16px;
                border: 1px solid #ffb300;   
                text-align: center;
                max-width: 90%;
                font-weight: 500;
                box-shadow: 0 2px 5px rgba(0,0,0,0.3);
            }

            #toast-container {
                position: absolute;
                top: 70px; 
                left: 50%;
                transform: translateX(-50%);
                z-index: 100;
                width: 90%;
                max-width: 400px;
                pointer-events: none; 
            }
            .toast {
                background: rgba(30, 30, 30, 0.95);
                color: #fff;
                padding: 12px 16px;
                border-radius: 8px;
                box-shadow: 0 4px 12px rgba(0,0,0,0.5);
                border-left: 4px solid #f44336; 
                margin-bottom: 10px;
                font-size: 0.9rem;
                display: flex;
                flex-direction: column;
                gap: 8px;
                opacity: 0;
                animation: slideDown 0.3s forwards, fadeOut 0.5s forwards 2.5s; 
                pointer-events: auto;
            }
            
            @keyframes slideDown { from { transform: translateY(-20px); opacity: 0; } to { transform: translateY(0); opacity: 1; } }
            @keyframes fadeOut { from { opacity: 1; } to { opacity: 0; visibility: hidden; } }

            .toast-btn {
                align-self: flex-end;
                background: transparent;
                border: 1px solid #666;
                color: #ccc;
                font-size: 0.75rem;
                padding: 4px 8px;
                border-radius: 4px;
                cursor: pointer;
            }
            .toast-btn:hover { background: #333; color: #fff; }

            #controls { background: #202c33; padding: 15px; border-top: 1px solid #374045; }
            
            button { 
                padding: 10px 20px; border-radius: 24px; border: none; font-size: 1rem; cursor: pointer; margin: 0 5px; font-weight: bold; transition: opacity 0.2s;
            }
            button:active { opacity: 0.7; }
            #btn-start { background: #00a884; color: #fff; }
            #btn-stop { background: #ef5350; color: #fff; display: none; }
            #btn-register { background: #3b4a54; color: #fff; font-size: 0.8rem; padding: 8px 15px; }
            #status { margin-bottom: 10px; font-size: 0.9rem; color: #8696a0; min-height: 1.2em; }
        </style>
    </head>
    <body>
        <div id="container">
            <header>
                <span>Team Chat AI</span>
                <button id="btn-register">ï¼‹ ãƒ¡ãƒ³ãƒãƒ¼è¿½åŠ </button>
            </header>
            
            <div id="toast-container"></div>
            <div id="chat-box"></div>
            
            <div id="controls">
                <div id="status">æ¥ç¶šå¾…æ©Ÿä¸­...</div>
                <button id="btn-start">ä¼šè©±ã‚’å§‹ã‚ã‚‹</button>
                <button id="btn-stop">çµ‚äº†ã™ã‚‹</button>
            </div>
        </div>

        <script>
            let socket;
            let audioContext;
            let processor;
            let sourceInput;
            let isRecording = false;
            
            const btnStart = document.getElementById('btn-start');
            const btnStop = document.getElementById('btn-stop');
            const btnRegister = document.getElementById('btn-register');
            const statusDiv = document.getElementById('status');
            const chatBox = document.getElementById('chat-box');
            const toastContainer = document.getElementById('toast-container');

            let audioQueue = [];
            let isPlaying = false;
            let currentSourceNode = null;
            let currentAiBubble = null;
            let muteUnregisteredWarning = false;

            function showToast(message) {
                if (muteUnregisteredWarning) return;
                const toast = document.createElement('div');
                toast.className = 'toast';
                const msgText = document.createElement('span');
                msgText.textContent = message;
                const muteBtn = document.createElement('button');
                muteBtn.className = 'toast-btn';
                muteBtn.textContent = "ä»Šå¾Œã“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã—ãªã„";
                muteBtn.onclick = () => {
                    muteUnregisteredWarning = true;
                    toast.style.display = 'none';
                };
                toast.appendChild(msgText);
                toast.appendChild(muteBtn);
                toastContainer.appendChild(toast);
                setTimeout(() => { if (toast.parentNode) toast.parentNode.removeChild(toast); }, 3000);
            }

            function logChat(role, text, speakerId = null) {
                const row = document.createElement('div');
                row.className = `row ${role}`;
                const bubble = document.createElement('div');

                if (role === 'system') {
                    bubble.className = 'system-bubble';
                    bubble.textContent = text;
                } else {
                    bubble.className = 'bubble';
                    bubble.textContent = text;
                    if (role === 'user' && speakerId) {
                        const nameLabel = document.createElement('div');
                        nameLabel.className = 'speaker-name';
                        nameLabel.textContent = speakerId; 
                        row.insertBefore(nameLabel, row.firstChild);
                        const idNum = speakerId.replace('User ', '');
                        if (!isNaN(idNum)) { row.classList.add(`user-type-${idNum}`); } 
                        else { row.classList.add('user-type-unknown'); }
                    } else if (role === 'ai') {
                         const nameLabel = document.createElement('div');
                        nameLabel.className = 'speaker-name';
                        nameLabel.textContent = "AI Assistant";
                        row.insertBefore(nameLabel, row.firstChild);
                    }
                }
                row.appendChild(bubble);
                chatBox.appendChild(row);
                chatBox.scrollTop = chatBox.scrollHeight;
                return bubble;
            }

            btnRegister.onclick = async () => {
                try {
                    await fetch('/enable-registration', { method: 'POST' });
                    statusDiv.textContent = "ğŸ†• æ–°è¦ãƒ¡ãƒ³ãƒãƒ¼ç™»éŒ²ãƒ¢ãƒ¼ãƒ‰";
                    statusDiv.style.color = "#00a884";
                    logChat('ai', "ã€ã‚·ã‚¹ãƒ†ãƒ ã€‘æ–°ã—ã„æ–¹ã®å£°ã‚’ç™»éŒ²ã—ã¾ã™ã€‚ãƒã‚¤ã‚¯ã«å‘ã‹ã£ã¦è©±ã—ã‹ã‘ã¦ãã ã•ã„ã€‚");
                } catch(e) { console.error(e); }
            };

            async function startRecording() {
                try {
                    statusDiv.textContent = "ã‚µãƒ¼ãƒãƒ¼æ¥ç¶šä¸­...";
                    const wsProtocol = window.location.protocol === 'https:' ? 'wss://' : 'ws://';
                    socket = new WebSocket(wsProtocol + window.location.host + '/ws');
                    socket.binaryType = 'arraybuffer';

                    socket.onopen = async () => {
                        statusDiv.textContent = "ğŸ™ï¸ æº–å‚™OK";
                        statusDiv.style.color = "#e9edef";
                        btnStart.style.display = 'none';
                        btnStop.style.display = 'inline-block';
                        await initAudioStream();
                    };

                    socket.onmessage = async (event) => {
                        if (event.data instanceof ArrayBuffer) {
                            audioQueue.push(event.data);
                            processAudioQueue();
                        } else {
                            const data = JSON.parse(event.data);
                            if (data.status === 'processing') statusDiv.textContent = data.message;
                            if (data.status === 'interrupt') stopAudioPlayback();
                            if (data.status === 'system_info') logChat('ai', data.message);

                            if (data.status === 'system_alert') {
                                if (data.alert_type === 'unregistered') showToast(data.message);
                                else if (data.alert_type === 'irrelevant') logChat('system', data.message);
                                statusDiv.textContent = "å¾…æ©Ÿä¸­...";
                            }

                            if (data.status === 'transcribed') logChat('user', data.question_text, data.speaker_id);
                            if (data.status === 'reply_chunk') {
                                if (!currentAiBubble) currentAiBubble = logChat('ai', ''); 
                                currentAiBubble.textContent += data.text_chunk;
                                chatBox.scrollTop = chatBox.scrollHeight;
                            }
                            if (data.status === 'complete') {
                                if (!currentAiBubble && data.answer_text) logChat('ai', data.answer_text);
                                currentAiBubble = null;
                                statusDiv.textContent = "ğŸ™ï¸ æº–å‚™OK";
                            }
                        }
                    };
                    socket.onclose = () => stopRecording();
                } catch (e) { console.error(e); }
            }

            async function initAudioStream() {
                audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
                const stream = await navigator.mediaDevices.getUserMedia({ audio: { channelCount: 1, echoCancellation: true, noiseSuppression: true, autoGainControl: true } });
                sourceInput = audioContext.createMediaStreamSource(stream);
                processor = audioContext.createScriptProcessor(4096, 1, 1);
                processor.onaudioprocess = (e) => {
                    if (!socket || socket.readyState !== WebSocket.OPEN) return;
                    socket.send(e.inputBuffer.getChannelData(0).buffer);
                };
                sourceInput.connect(processor);
                processor.connect(audioContext.destination);
                isRecording = true;
            }

            function stopRecording() {
                isRecording = false;
                if (sourceInput) sourceInput.disconnect();
                if (processor) processor.disconnect();
                if (audioContext) audioContext.close();
                if (socket) socket.close();
                btnStart.style.display = 'inline-block';
                btnStop.style.display = 'none';
                statusDiv.textContent = "åœæ­¢ä¸­";
            }

            function stopAudioPlayback() {
                if (currentSourceNode) { try { currentSourceNode.stop(); } catch(e){} currentSourceNode = null; }
                audioQueue = [];
                isPlaying = false;
            }

            async function processAudioQueue() {
                if (isPlaying || audioQueue.length === 0) return;
                isPlaying = true;
                const wavData = audioQueue.shift();
                try {
                    if (!audioContext || audioContext.state === 'closed') audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    const audioBuffer = await audioContext.decodeAudioData(wavData);
                    const source = audioContext.createBufferSource();
                    source.buffer = audioBuffer;
                    source.connect(audioContext.destination);
                    currentSourceNode = source;
                    source.onended = () => { currentSourceNode = null; isPlaying = false; processAudioQueue(); };
                    source.start(0);
                } catch(e) { isPlaying = false; currentSourceNode = null; }
            }

            btnStart.onclick = startRecording;
            btnStop.onclick = stopRecording;
        </script>
    </body>
    </html>
    """

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)