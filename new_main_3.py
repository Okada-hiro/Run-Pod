#new_main_2.pyã‹ã‚‰ã®é€²åŒ–ç³» å‰²ã‚Šè¾¼ã¿ãŒã§ãã‚‹

import uvicorn
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
import torch
import numpy as np
import asyncio
import logging
import sys
import os
import io
import re

# --- ãƒ­ã‚®ãƒ³ã‚°è¨­å®š ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# --- å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
try:
    from transcribe_func import GLOBAL_ASR_MODEL_INSTANCE
    from supporter_generator import generate_answer_stream
    from new_text_to_speech import synthesize_speech
    from new_speaker_filter import SpeakerGuard
except ImportError as e:
    logger.error(f"[ERROR] å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    sys.exit(1)

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š ---
PROCESSING_DIR = "incoming_audio"
os.makedirs(PROCESSING_DIR, exist_ok=True)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"Using Device: {DEVICE}")

app = FastAPI()
app.mount(f"/download", StaticFiles(directory=PROCESSING_DIR), name="download")

# SpeakerGuardåˆæœŸåŒ–
speaker_guard = SpeakerGuard()
NEXT_AUDIO_IS_REGISTRATION = False

# --- Silero VAD ã®ãƒ­ãƒ¼ãƒ‰ ---
logger.info("â³ Loading Silero VAD model...")
try:
    vad_model, utils = torch.hub.load(
        repo_or_dir='snakers4/silero-vad',
        model='silero_vad',
        force_reload=False,
        onnx=False
    )
    (get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils
    vad_model.to(DEVICE)
    logger.info("âœ… Silero VAD model loaded.")
except Exception as e:
    logger.critical(f"Silero VAD Load Failed: {e}")
    sys.exit(1)


# --- API: ç™»éŒ²ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ ---
@app.post("/enable-registration")
async def enable_registration():
    global NEXT_AUDIO_IS_REGISTRATION
    NEXT_AUDIO_IS_REGISTRATION = True
    logger.info("ã€ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ã€‘æ¬¡ã®ç™ºè©±ã‚’æ–°è¦è©±è€…ã¨ã—ã¦ç™»éŒ²ã—ã¾ã™")
    return {"message": "ç™»éŒ²ãƒ¢ãƒ¼ãƒ‰å¾…æ©Ÿä¸­"}


# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼: éŸ³å£°å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ---
async def process_voice_pipeline(audio_float32_np, websocket: WebSocket, chat_history: list):
    global NEXT_AUDIO_IS_REGISTRATION
    
    # SpeakerGuardç”¨ã« TensoråŒ– (1, samples)
    voice_tensor = torch.from_numpy(audio_float32_np).float().unsqueeze(0)

    # ---------------------------
    # 0. è©±è€…ç™»éŒ²ãƒ¢ãƒ¼ãƒ‰
    # ---------------------------
    if NEXT_AUDIO_IS_REGISTRATION:
        temp_reg_path = f"{PROCESSING_DIR}/reg_{id(audio_float32_np)}.wav"
        import soundfile as sf
        sf.write(temp_reg_path, audio_float32_np, 16000)
        
        success = await asyncio.to_thread(speaker_guard.register_new_speaker, temp_reg_path)
        NEXT_AUDIO_IS_REGISTRATION = False
        
        if success:
            await websocket.send_json({"status": "ignored", "message": "âœ… æ–°ã—ã„ãƒ¡ãƒ³ãƒãƒ¼ã‚’ç™»éŒ²ã—ã¾ã—ãŸï¼"})
        else:
            await websocket.send_json({"status": "error", "message": "ç™»éŒ²ã«å¤±æ•—ã—ã¾ã—ãŸ"})
        return

    # ---------------------------
    # 1. è©±è€…èªè­˜ (æœ€çµ‚ç¢ºèª)
    # ---------------------------
    # â€» å‰²ã‚Šè¾¼ã¿åˆ¤å®šã§ã™ã§ã«OKãŒå‡ºã¦ã„ã‚‹å ´åˆã‚‚å¤šã„ãŒã€å¿µã®ãŸã‚å…¨ãƒ‡ãƒ¼ã‚¿ã§æœ€çµ‚ç¢ºèª
    is_allowed = await asyncio.to_thread(speaker_guard.verify_tensor, voice_tensor)

    if not is_allowed:
        logger.info("[Access Denied] ç™»éŒ²ã•ã‚Œã¦ã„ãªã„è©±è€…ã§ã™ï¼ˆæœ€çµ‚åˆ¤å®šï¼‰ã€‚")
        await websocket.send_json({"status": "ignored", "message": "ğŸš« æœªç™»éŒ²ã®å£°ã§ã™ (ãƒ–ãƒ­ãƒƒã‚¯)"})
        return

    # ---------------------------
    # 2. Whisper æ–‡å­—èµ·ã“ã—
    # ---------------------------
    try:
        if GLOBAL_ASR_MODEL_INSTANCE is None:
            raise ValueError("Whisper Model not loaded")

        logger.info("[TASK] æ–‡å­—èµ·ã“ã—é–‹å§‹")
        segments = await asyncio.to_thread(
            GLOBAL_ASR_MODEL_INSTANCE.transcribe, 
            audio_float32_np
        )
        
        text = "".join([s[2] for s in GLOBAL_ASR_MODEL_INSTANCE.ts_words(segments)])
        
        if not text.strip():
            logger.info("[TASK] ç©ºã®èªè­˜çµæœ")
            return

        logger.info(f"[TASK] ãƒ†ã‚­ã‚¹ãƒˆ: {text}")
        await websocket.send_json({
            "status": "transcribed",
            "question_text": text
        })

        # ---------------------------
        # 3. LLM & TTS ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
        # ---------------------------
        await handle_llm_tts(text, websocket, chat_history)

    except Exception as e:
        logger.error(f"Pipeline Error: {e}", exc_info=True)
        await websocket.send_json({"status": "error", "message": "å‡¦ç†ã‚¨ãƒ©ãƒ¼"})


# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼: å›ç­”ç”Ÿæˆã¨éŸ³å£°åˆæˆ ---
async def handle_llm_tts(text: str, websocket: WebSocket, chat_history: list):
    text_buffer = ""
    sentence_count = 0
    full_answer = ""
    split_pattern = r'(?<=[ã€‚ï¼ï¼Ÿ\nã€])'

    # ç”Ÿæˆé–‹å§‹
    iterator = generate_answer_stream(text, history=chat_history)

    async def send_audio_chunk(phrase, idx):
        filename = f"resp_{idx}.wav"
        path = os.path.join(PROCESSING_DIR, filename)
        success = await asyncio.to_thread(synthesize_speech, phrase, path)
        if success:
            with open(path, 'rb') as f:
                wav_data = f.read()
            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡
            try:
                await websocket.send_bytes(wav_data)
            except RuntimeError:
                # æ¥ç¶šãŒåˆ‡ã‚Œã¦ã„ã‚‹å ´åˆãªã©
                pass

    try:
        for chunk in iterator:
            # â˜… ã“ã“ã«ã€Œæ–°ã—ã„å‰²ã‚Šè¾¼ã¿ã€ãŒã‚ã£ãŸå ´åˆã®ã‚­ãƒ£ãƒ³ã‚»ãƒ«å‡¦ç†ã‚’å…¥ã‚Œã‚‹ã“ã¨ã‚‚å¯èƒ½ã ãŒã€
            # ä»Šå›ã¯WebSocketãƒ«ãƒ¼ãƒ—å´ã§ç®¡ç†ã—ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒå†ç”Ÿã‚’æ­¢ã‚ã‚‹æ–¹å¼ã‚’æ¡ç”¨ã™ã‚‹ã€‚
            
            text_buffer += chunk
            full_answer += chunk
            
            if full_answer.strip() == "[SILENCE]":
                await websocket.send_json({"status": "ignored", "message": "ï¼ˆå¿œç­”ãªã—ï¼‰"})
                return

            sentences = re.split(split_pattern, text_buffer)
            if len(sentences) > 1:
                for sent in sentences[:-1]:
                    if sent.strip():
                        sentence_count += 1
                        # å­—å¹•é€ä¿¡
                        await websocket.send_json({"status": "reply_chunk", "text_chunk": sent})
                        # éŸ³å£°é€ä¿¡
                        await send_audio_chunk(sent, sentence_count)
                text_buffer = sentences[-1]
        
        if text_buffer.strip():
            sentence_count += 1
            await websocket.send_json({"status": "reply_chunk", "text_chunk": text_buffer})
            await send_audio_chunk(text_buffer, sentence_count)

        chat_history.append({"role": "user", "parts": [text]})
        chat_history.append({"role": "model", "parts": [full_answer]})
        
        await websocket.send_json({"status": "complete", "answer_text": full_answer})

    except Exception as e:
        logger.error(f"LLM/TTS Error: {e}")


# ---------------------------
# WebSocket ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ (Barge-inå¯¾å¿œ)
# ---------------------------
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    logger.info("[WS] Client Connected.")
    
    vad_iterator = VADIterator(
    vad_model, 
    threshold=0.5, 
    sampling_rate=16000, 
    min_silence_duration_ms=500, 
    speech_pad_ms=50
)
    audio_buffer = [] 
    is_speaking = False
    interruption_triggered = False # ä»Šå›ã®ç™ºè©±ã§ã™ã§ã«å‰²ã‚Šè¾¼ã¿æŒ‡ç¤ºã‚’å‡ºã—ãŸã‹
    
    # è¨­å®š
    WINDOW_SIZE_SAMPLES = 512 
    SAMPLE_RATE = 16000
    CHECK_SPEAKER_SAMPLES = 12000 # ç´„0.75ç§’æºœã¾ã£ãŸã‚‰è©±è€…ãƒã‚§ãƒƒã‚¯ã™ã‚‹
    
    chat_history = []

    try:
        while True:
            # 1. å—ä¿¡
            data_bytes = await websocket.receive_bytes()
            audio_chunk_np = np.frombuffer(data_bytes, dtype=np.float32).copy()
            
            # 2. 512ã‚µãƒ³ãƒ—ãƒ«åˆ†å‰²ãƒ«ãƒ¼ãƒ—
            offset = 0
            while offset + WINDOW_SIZE_SAMPLES <= len(audio_chunk_np):
                window_np = audio_chunk_np[offset : offset + WINDOW_SIZE_SAMPLES]
                offset += WINDOW_SIZE_SAMPLES
                
                # TensoråŒ– (1, 512)
                window_tensor = torch.from_numpy(window_np).unsqueeze(0).to(DEVICE)

                # VADåˆ¤å®š
                speech_dict = await asyncio.to_thread(vad_iterator, window_tensor, return_seconds=True)
                
                if speech_dict:
                    if "start" in speech_dict:
                        logger.info("ğŸ—£ï¸ Speech START")
                        is_speaking = True
                        interruption_triggered = False # ãƒªã‚»ãƒƒãƒˆ
                        audio_buffer = [window_np]
                        # UIæ›´æ–°: èãå–ã‚Šé–‹å§‹
                        await websocket.send_json({"status": "processing", "message": "ğŸ‘‚ èã„ã¦ã„ã¾ã™..."})
                    
                    elif "end" in speech_dict:
                        logger.info("ğŸ¤« Speech END")
                        if is_speaking:
                            is_speaking = False
                            audio_buffer.append(window_np)
                            
                            full_audio = np.concatenate(audio_buffer)
                            
                            # çŸ­ã™ãã‚‹ãƒã‚¤ã‚ºã¯ç„¡è¦–
                            if len(full_audio) / SAMPLE_RATE < 0.2:
                                logger.info("Noise detected (too short)")
                                await websocket.send_json({"status": "ignored", "message": "..."})
                            else:
                                await websocket.send_json({"status": "processing", "message": "ğŸ§  AIæ€è€ƒä¸­..."})
                                # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œï¼ˆéåŒæœŸã‚¿ã‚¹ã‚¯ã¨ã—ã¦æŠ•ã’ã‚‹ã¨ä¸¦åˆ—å‡¦ç†ã«ãªã‚‹ãŒã€
                                # ã“ã“ã§ã¯é †æ¬¡å‡¦ç†ã§ãƒãƒ£ãƒƒãƒˆã®æ•´åˆæ€§ã‚’ä¿ã¤ï¼‰
                                await process_voice_pipeline(full_audio, websocket, chat_history)
                            
                            audio_buffer = [] 
                
                else:
                    if is_speaking:
                        audio_buffer.append(window_np)
                        
                        # --- â˜…â˜…â˜… ãƒãƒ¼ã‚¸ã‚¤ãƒ³ï¼ˆå‰²ã‚Šè¾¼ã¿ï¼‰åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ â˜…â˜…â˜… ---
                        # ã¾ã å‰²ã‚Šè¾¼ã¿æŒ‡ç¤ºã‚’å‡ºã—ã¦ãŠã‚‰ãšã€ã‹ã¤ä¸€å®šé‡ï¼ˆ0.75ç§’åˆ†ãªã©ï¼‰éŸ³å£°ãŒæºœã¾ã£ãŸå ´åˆ
                        current_len = sum(len(c) for c in audio_buffer)
                        
                        if not interruption_triggered and not NEXT_AUDIO_IS_REGISTRATION and current_len > CHECK_SPEAKER_SAMPLES:
                            # æš«å®šãƒãƒƒãƒ•ã‚¡ã‚’çµåˆã—ã¦ãƒã‚§ãƒƒã‚¯
                            temp_audio = np.concatenate(audio_buffer)
                            temp_tensor = torch.from_numpy(temp_audio).float().unsqueeze(0)
                            
                            # è©±è€…ãƒã‚§ãƒƒã‚¯ (SpeakerGuard)
                            is_verified = await asyncio.to_thread(speaker_guard.verify_tensor, temp_tensor)
                            
                            if is_verified:
                                logger.info("âš¡ [Barge-in] æœ¬äººã®å£°ã‚’æ¤œçŸ¥ï¼å†ç”Ÿåœæ­¢æŒ‡ç¤ºã‚’é€ä¿¡ã—ã¾ã™ã€‚")
                                # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ã€ŒéŸ³å£°åœæ­¢ã€ã‚’æŒ‡ç¤º
                                await websocket.send_json({"status": "interrupt", "message": "ğŸ›‘ éŸ³å£°åœæ­¢"})
                                interruption_triggered = True
                            else:
                                # æœ¬äººã§ã¯ãªã„(é›‘éŸ³ã®å¯èƒ½æ€§) -> å‰²ã‚Šè¾¼ã¿æŒ‡ç¤ºã‚’é€ã‚‰ãªã„ï¼ˆç„¡è¦–ã—ã¦å†ç”Ÿç¶™ç¶šï¼‰
                                # â€»ãŸã ã—ã€æœ€çµ‚çš„ã«Speech ENDã¾ã§è¡Œã£ãŸã‚‰å†åº¦ãƒã‚§ãƒƒã‚¯ã•ã‚Œã¦ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹
                                pass

    except WebSocketDisconnect:
        logger.info("[WS] Disconnected")
    except Exception as e:
        logger.error(f"[WS ERROR] {e}", exc_info=True)
    finally:
        vad_iterator.reset_states()


# ---------------------------
# ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ (å­—å¹•ä¿®æ­£ & LINEé¢¨UI & å‰²ã‚Šè¾¼ã¿å¯¾å¿œ)
# ---------------------------
@app.get("/", response_class=HTMLResponse)
async def get_root():
    return """
    <!DOCTYPE html>
    <html lang="ja">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device.width, initial-scale=1.0">
        <title>Realtime Voice Chat âš¡</title>
        <style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; display: grid; place-items: center; min-height: 90vh; background: #202c33; color: #e9edef; margin: 0; }
            #container { background: #111b21; padding: 0; border-radius: 0; text-align: center; width: 100%; max-width: 600px; height: 100vh; display: flex; flex-direction: column; box-shadow: 0 0 20px rgba(0,0,0,0.5); }
            @media (min-width: 600px) {
                #container { height: 90vh; border-radius: 12px; }
            }
            
            header { background: #202c33; padding: 15px; border-bottom: 1px solid #374045; font-weight: bold; font-size: 1.1rem; display: flex; justify-content: space-between; align-items: center; }
            
            #chat-box { 
                flex: 1; overflow-y: auto; padding: 20px; 
                background-image: url("https://user-images.githubusercontent.com/15075759/28719144-86dc0f70-73b1-11e7-911d-60d70fcded21.png");
                background-repeat: repeat;
                background-size: 400px;
                background-color: #0b141a;
            }

            .row { display: flex; width: 100%; margin-bottom: 8px; }
            .row.ai { justify-content: flex-start; }
            .row.user { justify-content: flex-end; }
            
            .bubble { 
                padding: 8px 12px; border-radius: 8px; max-width: 75%; 
                font-size: 0.95rem; line-height: 1.4; position: relative; word-wrap: break-word;
                box-shadow: 0 1px 0.5px rgba(0,0,0,0.13);
            }
            .ai .bubble { background: #202c33; color: #e9edef; border-top-left-radius: 0; }
            .user .bubble { background: #005c4b; color: #e9edef; border-top-right-radius: 0; }
            
            #controls { background: #202c33; padding: 15px; border-top: 1px solid #374045; }
            
            button { 
                padding: 10px 20px; border-radius: 24px; border: none; font-size: 1rem; cursor: pointer; margin: 0 5px; font-weight: bold; transition: opacity 0.2s;
            }
            button:active { opacity: 0.7; }
            
            #btn-start { background: #00a884; color: #fff; }
            #btn-stop { background: #ef5350; color: #fff; display: none; }
            #btn-register { background: #3b4a54; color: #fff; font-size: 0.8rem; padding: 8px 15px; }

            #status { margin-bottom: 10px; font-size: 0.9rem; color: #8696a0; min-height: 1.2em; }
        </style>
    </head>
    <body>
        <div id="container">
            <header>
                <span>AI Agent</span>
                <button id="btn-register">ï¼‹ ãƒ¡ãƒ³ãƒãƒ¼è¿½åŠ </button>
            </header>
            
            <div id="chat-box"></div>
            
            <div id="controls">
                <div id="status">æ¥ç¶šå¾…æ©Ÿä¸­...</div>
                <button id="btn-start">ä¼šè©±ã‚’å§‹ã‚ã‚‹</button>
                <button id="btn-stop">çµ‚äº†ã™ã‚‹</button>
            </div>
        </div>

        <script>
            let socket;
            let audioContext;
            let processor;
            let sourceInput;
            let isRecording = false;
            
            const btnStart = document.getElementById('btn-start');
            const btnStop = document.getElementById('btn-stop');
            const btnRegister = document.getElementById('btn-register');
            const statusDiv = document.getElementById('status');
            const chatBox = document.getElementById('chat-box');

            // --- å†ç”Ÿç®¡ç†ç”¨å¤‰æ•° ---
            let audioQueue = [];
            let isPlaying = false;
            let currentSourceNode = null; // ç¾åœ¨å†ç”Ÿä¸­ã®AudioBufferSourceNode
            let currentAiBubble = null;   // å­—å¹•ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”¨DOM

            // --- UI Helper ---
            function logChat(role, text) {
                const row = document.createElement('div');
                row.className = `row ${role}`;
                const bubble = document.createElement('div');
                bubble.className = 'bubble';
                bubble.textContent = text;
                row.appendChild(bubble);
                chatBox.appendChild(row);
                chatBox.scrollTop = chatBox.scrollHeight;
                return bubble; 
            }

            // --- ãƒ¡ãƒ³ãƒãƒ¼ç™»éŒ² ---
            btnRegister.onclick = async () => {
                try {
                    await fetch('/enable-registration', { method: 'POST' });
                    statusDiv.textContent = "ğŸ†• æ–°è¦ãƒ¡ãƒ³ãƒãƒ¼ç™»éŒ²ãƒ¢ãƒ¼ãƒ‰";
                    statusDiv.style.color = "#00a884";
                    logChat('ai', "ã€ã‚·ã‚¹ãƒ†ãƒ ã€‘æ¬¡ã«è©±ã™äººã®å£°ã‚’ç™»éŒ²ã—ã¾ã™ã€‚ä½•ã‹è©±ã—ã‹ã‘ã¦ãã ã•ã„ã€‚");
                } catch(e) { console.error(e); }
            };

            // --- WebSocket & Audio ---
            async function startRecording() {
                try {
                    statusDiv.textContent = "ã‚µãƒ¼ãƒãƒ¼æ¥ç¶šä¸­...";
                    const wsProtocol = window.location.protocol === 'https:' ? 'wss://' : 'ws://';
                    socket = new WebSocket(wsProtocol + window.location.host + '/ws');
                    socket.binaryType = 'arraybuffer';

                    socket.onopen = async () => {
                        console.log("WS Connected");
                        statusDiv.textContent = "ğŸ™ï¸ æº–å‚™OKã€‚è©±ã—ã‹ã‘ã¦ãã ã•ã„";
                        statusDiv.style.color = "#e9edef";
                        
                        btnStart.style.display = 'none';
                        btnStop.style.display = 'inline-block';
                        
                        await initAudioStream();
                    };

                    socket.onmessage = async (event) => {
                        if (event.data instanceof ArrayBuffer) {
                            // éŸ³å£°ãƒ‡ãƒ¼ã‚¿å—ä¿¡ -> ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã—ã¦å†ç”Ÿå‡¦ç†ã¸
                            audioQueue.push(event.data);
                            processAudioQueue();
                        } else {
                            const data = JSON.parse(event.data);
                            
                            // ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
                            if (data.status === 'processing') {
                                statusDiv.textContent = data.message;
                                if (data.message.includes("èã„ã¦")) statusDiv.style.color = "#ef5350"; 
                                else if (data.message.includes("æ€è€ƒä¸­")) statusDiv.style.color = "#00a884";
                            }
                            
                            // â˜…â˜…â˜… å‰²ã‚Šè¾¼ã¿ (Interrupt) å‡¦ç† â˜…â˜…â˜…
                            if (data.status === 'interrupt') {
                                console.log("ğŸ›‘ Interrupt Signal Received!");
                                stopAudioPlayback(); // éŸ³å£°ã‚’å³æ™‚åœæ­¢
                                // å­—å¹•(bubble)ã¯ç¶­æŒã™ã‚‹ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¸Œæœ›ï¼‰
                            }

                            // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ï¼ˆå³å­—å¹•è¡¨ç¤ºï¼‰
                            if (data.status === 'transcribed') {
                                logChat('user', data.question_text);
                            }

                            // AIã®å›ç­”ï¼ˆå­—å¹•ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
                            if (data.status === 'reply_chunk') {
                                if (!currentAiBubble) {
                                    currentAiBubble = logChat('ai', ''); 
                                }
                                currentAiBubble.textContent += data.text_chunk;
                                chatBox.scrollTop = chatBox.scrollHeight;
                            }

                            // å®Œäº†æ™‚
                            if (data.status === 'complete') {
                                if (!currentAiBubble && data.answer_text) {
                                    logChat('ai', data.answer_text);
                                }
                                currentAiBubble = null; 
                                statusDiv.textContent = "ğŸ™ï¸ æº–å‚™OKã€‚è©±ã—ã‹ã‘ã¦ãã ã•ã„";
                                statusDiv.style.color = "#e9edef";
                            }

                            if (data.status === 'ignored') {
                                statusDiv.textContent = data.message;
                            }
                        }
                    };

                    socket.onclose = () => stopRecording();

                } catch (e) {
                    console.error(e);
                    statusDiv.textContent = "æ¥ç¶šã‚¨ãƒ©ãƒ¼";
                }
            }

            async function initAudioStream() {
                audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: { 
                        channelCount: 1, 
                        echoCancellation: true, 
                        noiseSuppression: true,
                        autoGainControl: true
                    } 
                });
                
                sourceInput = audioContext.createMediaStreamSource(stream);
                processor = audioContext.createScriptProcessor(4096, 1, 1);
                
                processor.onaudioprocess = (e) => {
                    if (!socket || socket.readyState !== WebSocket.OPEN) return;
                    const inputData = e.inputBuffer.getChannelData(0);
                    socket.send(inputData.buffer);
                };
                
                sourceInput.connect(processor);
                processor.connect(audioContext.destination);
                isRecording = true;
            }

            function stopRecording() {
                isRecording = false;
                if (sourceInput) sourceInput.disconnect();
                if (processor) processor.disconnect();
                if (audioContext) audioContext.close();
                if (socket) socket.close();
                
                btnStart.style.display = 'inline-block';
                btnStop.style.display = 'none';
                statusDiv.textContent = "åœæ­¢ä¸­";
            }

            // --- å†ç”Ÿãƒ­ã‚¸ãƒƒã‚¯ (å‰²ã‚Šè¾¼ã¿å¯¾å¿œç‰ˆ) ---
            
            // éŸ³å£°ã‚’å³æ™‚åœæ­¢ã—ã€ã‚­ãƒ¥ãƒ¼ã‚’ã‚¯ãƒªã‚¢ã™ã‚‹é–¢æ•°
            function stopAudioPlayback() {
                // 1. å†ç”Ÿä¸­ãªã‚‰æ­¢ã‚ã‚‹
                if (currentSourceNode) {
                    try {
                        currentSourceNode.stop();
                    } catch(e) {
                        // ã™ã§ã«æ­¢ã¾ã£ã¦ã„ã‚‹å ´åˆãªã©ã¯ç„¡è¦–
                    }
                    currentSourceNode = null;
                }
                // 2. å¾…æ©Ÿä¸­ã®éŸ³å£°ã‚’ç ´æ£„
                audioQueue = [];
                isPlaying = false;
                console.log("Audio Playback Cleared.");
            }

            async function processAudioQueue() {
                if (isPlaying || audioQueue.length === 0) return;
                isPlaying = true;
                const wavData = audioQueue.shift();
                
                try {
                    if (!audioContext || audioContext.state === 'closed') {
                         audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    }
                    const audioBuffer = await audioContext.decodeAudioData(wavData);
                    
                    const source = audioContext.createBufferSource();
                    source.buffer = audioBuffer;
                    source.connect(audioContext.destination);
                    
                    // ç¾åœ¨ã®ã‚½ãƒ¼ã‚¹ã¨ã—ã¦ä¿æŒï¼ˆstopç”¨ï¼‰
                    currentSourceNode = source;
                    
                    source.onended = () => {
                        // æ­£å¸¸çµ‚äº†ã—ãŸå ´åˆã®ã¿æ¬¡ã¸ï¼ˆstopã•ã‚ŒãŸå ´åˆã¯ã“ã“ã¯å‘¼ã°ã‚Œã‚‹ãŒã€queueã¯ç©ºã«ãªã£ã¦ã„ã‚‹ã¯ãšï¼‰
                        currentSourceNode = null;
                        isPlaying = false;
                        processAudioQueue();
                    };
                    source.start(0);
                } catch(e) {
                    console.error("å†ç”Ÿã‚¨ãƒ©ãƒ¼", e);
                    isPlaying = false;
                    currentSourceNode = null;
                }
            }

            btnStart.onclick = startRecording;
            btnStop.onclick = stopRecording;
        </script>
    </body>
    </html>
    """

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)